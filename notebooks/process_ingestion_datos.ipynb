{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tb2rs484mpjf17il45hkcf46dapon1qv', 1733413267, '2024-12-05 15:41:07')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyTigerGraph as tg\n",
    "from pydantic import BaseModel, ValidationError, model_validator\n",
    "from typing import Literal, Dict, Optional\n",
    "import importlib.util\n",
    "import pyTigerGraph as tg\n",
    "import os\n",
    "import json\n",
    "from templates.create_local_graph_schema import create_graph_schema_template\n",
    "from templates.create_abs_data_source import create_abs_data_source_template\n",
    "from templates.create_postgre_data_source import create_postgre_data_source_template\n",
    "from templates.create_loading_jobs import create_loading_jobs_template\n",
    "from templates.create_loading_jobs_for_local_files import create_loading_jobs_template_local_files\n",
    "from templates.abs_data_source import abs_file_path_template\n",
    "\n",
    "\n",
    "hostName = \"https://2cc2f8bde8df444bb60c6fb83491bb8c.i.tgcloud.io\"\n",
    "graphName = \"VWG\"\n",
    "secret = \"m2p8nba0uab7dtthbn4o1b30r8tqgg9a\"\n",
    "userName = \"user_1\"\n",
    "password = \"A1z2e3r4*\"\n",
    "conn = tg.TigerGraphConnection(host=hostName, graphname=graphName, username=userName, password=password)\n",
    "conn.getToken(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_dict = {\n",
    "    \"vertex_data\": r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\data\\Europe\\biggest_europe_cities.csv\",  # Chemin vers le fichier de données pour les sommets\n",
    "    \"edge_data\": r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\data\\Europe\\conexiones_biggest_europe_cities.csv\"       # Chemin vers le fichier de données pour les arêtes\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(module_name: str, file_path: str):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "\n",
    "\n",
    "def load_queries(folder_name: str = 'queries'):\n",
    "    \"\"\"This function loads all templates called \"query\" inside the each python file on folder folder_name.\n",
    "    NOTE! the name of the .py file should match the query name\n",
    "\n",
    "    Args:\n",
    "        folder_name (str, optional): Folder to retrieve queries. Defaults to 'queries'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with key = query name, value = actual query template\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    for filename in os.listdir(folder_name):\n",
    "        if filename.endswith('.py'):\n",
    "            file_path = os.path.join(folder_name, filename)\n",
    "            module_name = filename[:-3]\n",
    "            module = load_module(module_name, file_path)\n",
    "            if hasattr(module, 'query'):\n",
    "                queries[module_name] = getattr(module, 'query')\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ABSConfig(BaseModel):\n",
    "    \"\"\"Required attributes for azure blob storage data source.\"\"\"\n",
    "    storageaccount: str\n",
    "    container: str\n",
    "    connection_key: str\n",
    "\n",
    "\n",
    "\n",
    "class PostgreConfig(BaseModel):\n",
    "    host: str\n",
    "    port: int\n",
    "    db_user: str\n",
    "    db_password: str\n",
    "    db_name: str\n",
    "\n",
    "\n",
    "\n",
    "class PostgreDataSourceConfig(BaseModel):\n",
    "    \"\"\"Validation model for when data source is postgre SQL database\n",
    "    Raises:\n",
    "        ValueError: when there are missing keys\n",
    "    \"\"\"\n",
    "    data_source_type: str\n",
    "    postgre_config: Optional[PostgreConfig] = None\n",
    "    @model_validator(mode='before')\n",
    "    def check_postgre_config(cls, values):\n",
    "        data_source_type = values.get('data_source_type')\n",
    "        postgre_config = values.get('postgre_config')\n",
    "        if data_source_type == 'postgre' and not postgre_config:\n",
    "            raise ValueError('If data_source_type is postgre, you must provide the required keys in postgre_config.')\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "class ABSDataSourceConfig(BaseModel):\n",
    "    \"\"\"Validation model for when data source is azure blob storage\n",
    "    Raises:\n",
    "        ValueError: when there are missing keys\n",
    "    \"\"\"\n",
    "    data_source_type: str\n",
    "    abs_config: Optional[ABSConfig] = None\n",
    "    @model_validator(mode='before')\n",
    "    def check_abs_config(cls, values):\n",
    "        data_source_type = values.get('data_source_type')\n",
    "        abs_config = values.get('abs_config')\n",
    "        if data_source_type == \"abs\" and not abs_config:\n",
    "            raise ValueError('If data_source_type is abs, you must provide the required keys in abs_config.')\n",
    "        return values\n",
    "\n",
    "class GraphGenerator():\n",
    "    def __init__(self, conn: tg.TigerGraphConnection, \n",
    "                graph_name: str, \n",
    "                data_source_dict: Dict,\n",
    "                data_source_type: Literal['local_files', 'abs', 'postgre'] = 'local_files',\n",
    "                config_data: Optional[dict] = None):\n",
    "        \n",
    "        self.graph_name = graph_name\n",
    "        self.data_source_dict = data_source_dict\n",
    "        self.conn = conn \n",
    "        self.data_source_type = data_source_type\n",
    "\n",
    "        # if data source is distinct from local files, check if all required keys are present in the config_data dict\n",
    "        if self.data_source_type == 'abs':\n",
    "            self.data_source = self.graph_name+'_abs_data_source'\n",
    "            try:\n",
    "                self.config = ABSDataSourceConfig(\n",
    "                    data_source_type=data_source_type,\n",
    "                    abs_config=config_data\n",
    "                )\n",
    "            except ValidationError as e:\n",
    "                print(\"Validation error:\", e)\n",
    "        elif self.data_source_type == 'postgre':\n",
    "            self.data_source = self.graph_name+'_postgre_data_source'\n",
    "            try:\n",
    "                self.config = PostgreDataSourceConfig(\n",
    "                    data_source_type = data_source_type,\n",
    "                    postgre_config = config_data)\n",
    "            except ValidationError as e:\n",
    "                print(\"Validation error:\", e)\n",
    "\n",
    "\n",
    "\n",
    "    def create_local_graph(self):\n",
    "        create_graph_statement = create_graph_schema_template.substitute({'graph_name': self.graph_name, \n",
    "                                                                        'schema_change_job': 'schema_change_job_'+str(self.graph_name)})\n",
    "        try:\n",
    "            print(self.conn.gsql(create_graph_statement))\n",
    "            # update conn object to point to new graph\n",
    "            self.conn.graphname = self.graph_name \n",
    "            self.conn.echo() \n",
    "            #create secret and save it as a file - theres prob a better way to do this \n",
    "            secret = self.conn.createSecret(alias=self.graph_name+'_secret')\n",
    "            self.conn.gsqlSecret = secret\n",
    "            info = {}\n",
    "            info['graphname'] = self.graph_name\n",
    "            info['secret'] = secret\n",
    "            with open(f\"credentials_{self.graph_name}.json\", 'w') as f:\n",
    "                json.dump(info, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Something went wrong while creating the graph: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def create_data_source(self):\n",
    "        # if source is local files, we dont need to create a datasource\n",
    "        if self.data_source_type == 'local_files':\n",
    "            return \n",
    "        if self.data_source_type == 'abs':\n",
    "            create_data_source_statement = create_abs_data_source_template.substitute({\n",
    "                    'graph_name': self.graph_name, \n",
    "                    'abs_datasource': self.data_source,\n",
    "                    'azure_key': self.config.abs_config.connection_key})\n",
    "        elif self.data_source_type == 'postgre':\n",
    "            create_data_source_statement = create_postgre_data_source_template.substitute({\n",
    "                    'graph_name': self.graph_name, \n",
    "                    'postgre_datasource': self.data_source,\n",
    "                    'host': self.config.postgre_config.host,\n",
    "                    'port': self.config.postgre_config.port,\n",
    "                    'db_user': self.config.postgre_config.db_user,\n",
    "                    'db_password': self.config.postgre_config.db_password,\n",
    "                    'db_name': self.config.postgre_config.db_name})\n",
    "        try:\n",
    "            current_ds = self.conn.gsql(f\"USE GRAPH {self.graph_name} SHOW DATA_SOURCE *\")\n",
    "            if self.data_source not in current_ds:\n",
    "                print(self.conn.gsql(create_data_source_statement))\n",
    "            else:\n",
    "                print(f\"Data source {self.data_source} already exists for graph {self.graph_name}!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Something went wrong while creating the datasource: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def create_loading_jobs(self):\n",
    "        self.data_source_dict['graph_name'] = self.graph_name\n",
    "\n",
    "        if self.data_source_type == 'abs':\n",
    "            # if datasource is ABS we need to construct the absolute path to each file in ABS\n",
    "            main_path = abs_file_path_template.substitute({'abs_datasource': '$'+self.data_source,\n",
    "                                                    'storage_account': self.config.abs_config.storageaccount,\n",
    "                                                    'container': self.config.abs_config.container})\n",
    "            self.data_source_dict = {key: '\\\"'+main_path+value+'\\\"' for key, value in self.data_source_dict.items() if key !='graph_name'}\n",
    "            create_loading_jobs_statement = create_loading_jobs_template.substitute(self.data_source_dict)\n",
    "\n",
    "        elif self.data_source_type == 'postgre':\n",
    "            # in case of postgre, each data_sources[key] = query so no changes are needed\n",
    "            create_loading_jobs_statement = create_loading_jobs_template.substitute(self.data_source_dict)\n",
    "\n",
    "        elif self.data_source_type == 'local_files':\n",
    "            # for local files we don't need to define each filename = datasource, we do it from the command runLoadingJobFromFile\n",
    "            create_loading_jobs_statement = create_loading_jobs_template_local_files.substitute({'graph_name': \n",
    "                                                                                                self.graph_name})\n",
    "\n",
    "        # we'll use this to iterate through the loading jobs :-) so delete graphname so it doesn't explode\n",
    "        del self.data_source_dict['graph_name']\n",
    "        try:\n",
    "            print(self.conn.gsql(create_loading_jobs_statement))\n",
    "        except Exception as e:\n",
    "            print(f\"Something went wrong while creating the loading jobs: {str(e)}\")\n",
    "\n",
    "\n",
    "    def load_from_files(self):\n",
    "        created_jobs = self.conn.gsql(f\"USE GRAPH {self.graph_name} SHOW JOB *\")\n",
    "        jobs_to_rerun = []\n",
    "        for job, file_name in self.data_source_dict.items():\n",
    "            job_name = job.replace('_data', '')\n",
    "            if job_name in created_jobs:\n",
    "                try:\n",
    "                    print(f\"---- Uploading file {file_name} for job {job_name} ---- \\n\")\n",
    "                    print(self.conn.runLoadingJobWithFile(filePath = file_name, \n",
    "                                                        fileTag = \"MyDataSource\" , \n",
    "                                                        jobName = f\"load_{job_name}\", timeout=32000))\n",
    "                except Exception as e:\n",
    "                    print(f\"Job {job_name} went bad: {str(e)}\")\n",
    "                    jobs_to_rerun.append(job_name)\n",
    "            else:\n",
    "                print(f\"Job {job_name} doesn't exist.\")\n",
    "        print(f\"Jobs that failed: {jobs_to_rerun}\")\n",
    "\n",
    "\n",
    "\n",
    "    def load_from_abs(self):\n",
    "        jobs = list(self.data_source_dict.keys())\n",
    "        jobs = ['load_'+j.replace('_data', '') for j in jobs]\n",
    "        created_jobs = self.conn.gsql(f\"USE GRAPH {self.graph_name} SHOW JOB *\")\n",
    "        jobs_to_rerun = []\n",
    "        for j in jobs:\n",
    "            if j in created_jobs:\n",
    "                execute_statement = f\"USE GRAPH {self.graph_name} RUN LOADING JOB {j}\"\n",
    "                print(f\"---- Running job {j} ---- \\n\")\n",
    "                try:\n",
    "                    self.conn.gsql(execute_statement)\n",
    "                except Exception as e:\n",
    "                    print(f\"Job {j} went bad: {str(e)}\")\n",
    "                    jobs_to_rerun.append(j)\n",
    "        print(f\"Jobs that failed: {jobs_to_rerun}\")\n",
    "\n",
    "\n",
    "\n",
    "    def load_from_postgre(self):\n",
    "        \"\"\"I am not sure if we need another method for postgre stuff.\n",
    "        I think we prob could use load_from_abs since the loading jobs are already created\n",
    "        and pointing to the sql queries, so no further changes would be needed (?). \n",
    "        I'll leave it here just in case\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def execute_loading_jobs(self):\n",
    "        if self.data_source_type == 'abs':\n",
    "            self.load_from_abs()\n",
    "        elif self.data_source_type == 'local_files':\n",
    "            self.load_from_files()\n",
    "\n",
    "\n",
    "    def create_queries(self):\n",
    "        queries_to_install = load_queries()\n",
    "        for name, statement in queries_to_install.items():\n",
    "            st = statement.substitute({'graph_name': self.graph_name})\n",
    "            try:\n",
    "                print(f\"---- Creating query {name} ---- \\n\")\n",
    "                conn.gsql(st)\n",
    "                print(f\"---- Installing query {name} ---- \\n\")\n",
    "                conn.gsql(f\"USE GRAPH {self.graph_name} INSTALL QUERY {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Query {name} went bad: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        self.create_local_graph()\n",
    "        self.create_data_source()\n",
    "        self.create_loading_jobs()\n",
    "        self.execute_loading_jobs()\n",
    "        self.create_queries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_generator = GraphGenerator(\n",
    "    conn=conn,\n",
    "    graph_name=\"VWG\",\n",
    "    data_source_dict=data_source_dict,\n",
    "    data_source_type='local_files'  # Ou 'abs' ou 'postgre' selon votre source de données\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong while creating the loading jobs: Using graph 'VWG'\n",
      "Semantic Check Fails: The vertex type country does not exist in the graph VWG\n",
      "Failed to create loading jobs: [load_country].\n"
     ]
    }
   ],
   "source": [
    "graph_generator.create_loading_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
