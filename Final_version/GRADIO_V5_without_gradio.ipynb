{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io', port=9000): Max retries exceeded with url: /requesttoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001D86CB303E0>: Failed to resolve '0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001D86CB30470>: Failed to resolve '0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io', port=9000): Max retries exceeded with url: /requesttoken?secret=234upsro22u18q607sc3dos2fg2us2iq (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001D86CB30470>: Failed to resolve '0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphAuth.py:266\u001b[0m, in \u001b[0;36mpyTigerGraphAuth.getToken\u001b[1;34m(self, secret, setToken, lifetime)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestppUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/requesttoken?secret=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msecret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&lifetime=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     mainVer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m# Can't use _verGreaterThan4_0 to check version since you need to set a token for that\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io', port=9000): Max retries exceeded with url: /requesttoken?secret=234upsro22u18q607sc3dos2fg2us2iq (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001D86CB30470>: Failed to resolve '0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# tigergraph_insights_map = \"https://tools.tgcloud.io/insights/app/qepJkoYLXfcWTB4d5ExgVT/page/4NjQNvfLhhTXHgQZBkyxtM/widgetShare/6wci2PjrXvzciTfmrH9pQY?domain=d6e9eef375704c8893b47bdc6132b082.i&orgName=lis-gordias&clusterid=507badf1-6ed3-4a79-9aaa-82489ed609ef&TigerGraphToken=e4120fbd-1fb5-4383-a727-33fb7032eb53\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m graph \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mTigerGraphConnection(host\u001b[38;5;241m=\u001b[39mhostName, graphname\u001b[38;5;241m=\u001b[39mgraphName)\n\u001b[1;32m---> 13\u001b[0m authToken \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetToken\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m authToken \u001b[38;5;241m=\u001b[39m authToken[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecret token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthToken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphAuth.py:276\u001b[0m, in \u001b[0;36mpyTigerGraphAuth.getToken\u001b[1;34m(self, secret, setToken, lifetime)\u001b[0m\n\u001b[0;32m    274\u001b[0m                 success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TigerGraphException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot request a token with username/password for versions < 3.5.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphAuth.py:260\u001b[0m, in \u001b[0;36mpyTigerGraphAuth.getToken\u001b[1;34m(self, secret, setToken, lifetime)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifetime:\n\u001b[0;32m    258\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlifetime\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(lifetime)\n\u001b[1;32m--> 260\u001b[0m res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestppUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/requesttoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    262\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    263\u001b[0m mainVer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io', port=9000): Max retries exceeded with url: /requesttoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001D86CB303E0>: Failed to resolve '0eede95b6ddc4300bdc7b3103098e51c.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "#CLUSTER GORDIAS LIS \n",
    "import pyTigerGraph as tg\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "graph = tg.TigerGraphConnection(host=hostName, graphname=graphName)\n",
    "\n",
    "authToken = graph.getToken(secret)\n",
    "authToken = authToken[0]\n",
    "print(f\"secret token: {authToken}\")\n",
    "conn = tg.TigerGraphConnection(host=hostName, graphname=graphName, username=userName, password=password, apiToken=authToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GraphName': 'VWG',\n",
       " 'VertexTypes': [{'Config': {'STATS': 'OUTDEGREE_BY_EDGETYPE'},\n",
       "   'Attributes': [{'AttributeType': {'Name': 'FLOAT'},\n",
       "     'AttributeName': 'LoadCapacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'UnloadCapacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Stock'},\n",
       "    {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'Carga'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Capacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'latitude'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'longitude'}],\n",
       "   'PrimaryId': {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'id'},\n",
       "   'Name': 'Nodes'}],\n",
       " 'EdgeTypes': [{'IsDirected': False,\n",
       "   'ToVertexTypeName': 'Nodes',\n",
       "   'Config': {},\n",
       "   'Attributes': [{'AttributeType': {'Name': 'FLOAT'},\n",
       "     'AttributeName': 'Price'},\n",
       "    {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'Carga'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Capacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Daily_movement'}],\n",
       "   'FromVertexTypeName': 'Nodes',\n",
       "   'Name': 'distribute_to'}],\n",
       " 'UDTs': []}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.getSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://director-lisdatasolutions.i.tgcloud.io:443/gsqlserver/gsql/file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgsql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43mUSE GRAPH transport_demo\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43mCREATE QUERY tg_astar (VERTEX source_vertex, VERTEX target_vertex, SET<STRING> e_type_set,\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43mSTRING weight_type, STRING latitude, STRING longitude,\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43mSTRING weight_attribute, BOOL print_stats = False) SYNTAX V1 \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43mTYPEDEF TUPLE<FLOAT dist, VERTEX v> pathTuple;    # <shotest distance, parent node>\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43mHeapAccum<pathTuple>(1, dist ASC) @@find_min_v_heap;  # retain 1 shortest path\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43mHeapAccum<pathTuple>(1, dist ASC) @min_dist_heap;\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43mOrAccum @or_visited, @@or_valid_path_exists;\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;43mListAccum<VERTEX> @@tmp_list;  # the optimal node\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;43mSumAccum<FLOAT> @@sum_total_dist;  # the shortest distance\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;43mSetAccum<EDGE> @@display_edge_set;\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;43mSetAccum<VERTEX> @@display_node_set;\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;43mINT hop;\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;43mFLOAT x1,y1;\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;43m# Check weight_type parameter\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;43mIF weight_type NOT IN (\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) THEN\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;43m    PRINT \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_type must be INT, FLOAT, or DOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m AS errMsg;\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;43m    RETURN;\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;43mEND;\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;43m# record target latitude and longitude\u001b[39;49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;43mTgt = \u001b[39;49m\u001b[38;5;132;43;01m{target_vertex}\u001b[39;49;00m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;43mTgt = SELECT s\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;43m      FROM Tgt:s\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;43m      POST-ACCUM x1 = s.getAttr(latitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;43m\t         y1 = s.getAttr(longitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m);\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;43mStart = \u001b[39;49m\u001b[38;5;132;43;01m{source_vertex}\u001b[39;49;00m\u001b[38;5;124;43m;   # the optimal node\u001b[39;49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;43mOpt = \u001b[39;49m\u001b[38;5;132;43;01m{source_vertex}\u001b[39;49;00m\u001b[38;5;124;43m;    # all of the optimal nodes\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;43mStart = SELECT s\u001b[39;49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;43m        FROM Start:s\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;43m        ACCUM s.@or_visited = True,\u001b[39;49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;43m              s.@min_dist_heap = pathTuple(0,s);\u001b[39;49m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;43m# run aster to find shortest distance greedily\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;43mWHILE Opt.size() > 0 DO\u001b[39;49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;43m    # find the node with shortest distance\u001b[39;49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;43m    Opt = SELECT t\u001b[39;49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;43m          FROM Opt:s-(e_type_set:e)-> :t\u001b[39;49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;43m          WHERE t.@or_visited == False\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;43m\t  ACCUM\u001b[39;49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;43m              # we use Haversine formula as the heuristic function here\u001b[39;49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;43m              CASE weight_type WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;43m                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;43m\t\t  +  tg_GetDistance(t.getAttr(latitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),t.getAttr(longitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),x1,y1),s)\u001b[39;49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;43m              WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;43m                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;43m\t\t  +  tg_GetDistance(t.getAttr(latitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),t.getAttr(longitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),x1,y1),s)\u001b[39;49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;43m              WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;43m                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;43m\t\t  +  tg_GetDistance(t.getAttr(latitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),t.getAttr(longitude,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),x1,y1),s)\u001b[39;49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;43m              END;\u001b[39;49m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;43m    Opt = SELECT t\u001b[39;49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;43m          FROM Start:s-(e_type_set:e)-> :t\u001b[39;49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;43m          WHERE t.@or_visited == False\u001b[39;49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;43m          POST-ACCUM @@find_min_v_heap += pathTuple(t.@min_dist_heap.top().dist,t);\u001b[39;49m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;43m    @@tmp_list.clear();\u001b[39;49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;43m    IF @@find_min_v_heap.size() > 0 THEN\u001b[39;49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;43m        @@tmp_list += @@find_min_v_heap.pop().v;\u001b[39;49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;43m    END;\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;43m    Opt = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m@@tmp_list};\u001b[39;49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;43m    Start = Opt UNION Start;\u001b[39;49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;43m    Opt = SELECT t\u001b[39;49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;43m          FROM Opt:t\u001b[39;49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;43m\t  POST-ACCUM\u001b[39;49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;43m\t      t.@or_visited += True;\u001b[39;49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;43m              # Determine if it is the target point and terminate the loop if it is\u001b[39;49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;43m              IF @@tmp_list.get(0) == target_vertex THEN\u001b[39;49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;43m                  BREAK;\u001b[39;49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;43m              END;\u001b[39;49m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;43mEND;\u001b[39;49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;43m# The test is whether there is a path between two points\u001b[39;49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;43mStart = \u001b[39;49m\u001b[38;5;132;43;01m{target_vertex}\u001b[39;49;00m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;43mStart = SELECT s\u001b[39;49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;43m        FROM Start:s\u001b[39;49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;43m        POST-ACCUM @@or_valid_path_exists += s.@min_dist_heap.size() > 0,\u001b[39;49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;43m                   @@display_node_set += s;\u001b[39;49m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;43mIF @@or_valid_path_exists THEN\u001b[39;49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;43m    # find path\u001b[39;49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;43m    WHILE Start.size() > 0 DO\u001b[39;49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;43m        Start =\u001b[39;49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;43m\tSELECT t\u001b[39;49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;43m\tFROM Start:s-(e_type_set:e)-> :t\u001b[39;49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;43m        WHERE t == s.@min_dist_heap.top().v\u001b[39;49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;43m        ACCUM\u001b[39;49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;43m\t    @@display_edge_set += e,\u001b[39;49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;43m            CASE weight_type WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;43m                @@sum_total_dist += e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;43m            WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;43m                @@sum_total_dist += e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;43m            WHEN \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m THEN\u001b[39;49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;43m                @@sum_total_dist += e.getAttr(weight_attribute, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOUBLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;43m            END,\u001b[39;49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;43m            @@display_node_set += t;\u001b[39;49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;43m            hop = hop + 1;\u001b[39;49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;43m    END;\u001b[39;49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;43m    hop = hop - 1;\u001b[39;49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;43m    PRINT @@sum_total_dist;\u001b[39;49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;43m    PRINT hop;\u001b[39;49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;43m    IF print_stats THEN\u001b[39;49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;43m        tmp = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m@@display_node_set};\u001b[39;49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;43m        PRINT @@display_edge_set,tmp;\u001b[39;49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;43m    END;\u001b[39;49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;43mELSE\u001b[39;49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;43m    PRINT \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNo viable path found.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;43mEND;\u001b[39;49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphGSQL.py:76\u001b[0m, in \u001b[0;36mpyTigerGraphGSQL.gsql\u001b[1;34m(self, query, graphname, options)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(graphname)\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLOBAL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(graphname)\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     74\u001b[0m     graphname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_req\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgsUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gsqlserver/gsql/file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m                \u001b[49m\u001b[43mauthMode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpwd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipCheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m                \u001b[49m\u001b[43mjsonResponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     84\u001b[0m     ret \u001b[38;5;241m=\u001b[39m clean_res(res)\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:295\u001b[0m, in \u001b[0;36mpyTigerGraphBase._req\u001b[1;34m(self, method, url, authMode, headers, data, resKey, skipCheck, params, strictJson, jsonData, jsonResponse)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(method, url, headers\u001b[38;5;241m=\u001b[39m_headers, data\u001b[38;5;241m=\u001b[39m_data, params\u001b[38;5;241m=\u001b[39mparams, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m--> 295\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsonResponse:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://director-lisdatasolutions.i.tgcloud.io:443/gsqlserver/gsql/file"
     ]
    }
   ],
   "source": [
    "result = conn.gsql(\"\"\"\n",
    "USE GRAPH transport_demo\n",
    "CREATE QUERY tg_astar (VERTEX source_vertex, VERTEX target_vertex, SET<STRING> e_type_set,\n",
    "STRING weight_type, STRING latitude, STRING longitude,\n",
    "STRING weight_attribute, BOOL print_stats = False) SYNTAX V1 {\n",
    "\n",
    "TYPEDEF TUPLE<FLOAT dist, VERTEX v> pathTuple;    # <shotest distance, parent node>\n",
    "HeapAccum<pathTuple>(1, dist ASC) @@find_min_v_heap;  # retain 1 shortest path\n",
    "HeapAccum<pathTuple>(1, dist ASC) @min_dist_heap;\n",
    "OrAccum @or_visited, @@or_valid_path_exists;\n",
    "ListAccum<VERTEX> @@tmp_list;  # the optimal node\n",
    "SumAccum<FLOAT> @@sum_total_dist;  # the shortest distance\n",
    "SetAccum<EDGE> @@display_edge_set;\n",
    "SetAccum<VERTEX> @@display_node_set;\n",
    "INT hop;\n",
    "FLOAT x1,y1;\n",
    "\n",
    "# Check weight_type parameter\n",
    "IF weight_type NOT IN (\"INT\", \"FLOAT\", \"DOUBLE\") THEN\n",
    "    PRINT \"weight_type must be INT, FLOAT, or DOUBLE\" AS errMsg;\n",
    "    RETURN;\n",
    "END;\n",
    "\n",
    "# record target latitude and longitude\n",
    "Tgt = {target_vertex};\n",
    "Tgt = SELECT s\n",
    "      FROM Tgt:s\n",
    "      POST-ACCUM x1 = s.getAttr(latitude,\"FLOAT\"),\n",
    "\t         y1 = s.getAttr(longitude,\"FLOAT\");\n",
    "\n",
    "Start = {source_vertex};   # the optimal node\n",
    "Opt = {source_vertex};    # all of the optimal nodes\n",
    "\n",
    "Start = SELECT s\n",
    "        FROM Start:s\n",
    "        ACCUM s.@or_visited = True,\n",
    "              s.@min_dist_heap = pathTuple(0,s);\n",
    "\n",
    "# run aster to find shortest distance greedily\n",
    "WHILE Opt.size() > 0 DO\n",
    "    # find the node with shortest distance\n",
    "    Opt = SELECT t\n",
    "          FROM Opt:s-(e_type_set:e)-> :t\n",
    "          WHERE t.@or_visited == False\n",
    "\t  ACCUM\n",
    "              # we use Haversine formula as the heuristic function here\n",
    "              CASE weight_type WHEN \"INT\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"INT\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              WHEN \"FLOAT\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"FLOAT\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              WHEN \"DOUBLE\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"DOUBLE\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              END;\n",
    "\n",
    "    Opt = SELECT t\n",
    "          FROM Start:s-(e_type_set:e)-> :t\n",
    "          WHERE t.@or_visited == False\n",
    "          POST-ACCUM @@find_min_v_heap += pathTuple(t.@min_dist_heap.top().dist,t);\n",
    "\n",
    "    @@tmp_list.clear();\n",
    "    IF @@find_min_v_heap.size() > 0 THEN\n",
    "        @@tmp_list += @@find_min_v_heap.pop().v;\n",
    "    END;\n",
    "\n",
    "    Opt = {@@tmp_list};\n",
    "    Start = Opt UNION Start;\n",
    "    Opt = SELECT t\n",
    "          FROM Opt:t\n",
    "\t  POST-ACCUM\n",
    "\t      t.@or_visited += True;\n",
    "              # Determine if it is the target point and terminate the loop if it is\n",
    "              IF @@tmp_list.get(0) == target_vertex THEN\n",
    "                  BREAK;\n",
    "              END;\n",
    "\n",
    "END;\n",
    "# The test is whether there is a path between two points\n",
    "Start = {target_vertex};\n",
    "Start = SELECT s\n",
    "        FROM Start:s\n",
    "        POST-ACCUM @@or_valid_path_exists += s.@min_dist_heap.size() > 0,\n",
    "                   @@display_node_set += s;\n",
    "\n",
    "IF @@or_valid_path_exists THEN\n",
    "    # find path\n",
    "    WHILE Start.size() > 0 DO\n",
    "        Start =\n",
    "\tSELECT t\n",
    "\tFROM Start:s-(e_type_set:e)-> :t\n",
    "        WHERE t == s.@min_dist_heap.top().v\n",
    "        ACCUM\n",
    "\t    @@display_edge_set += e,\n",
    "            CASE weight_type WHEN \"INT\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"INT\")\n",
    "            WHEN \"FLOAT\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"FLOAT\")\n",
    "            WHEN \"DOUBLE\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"DOUBLE\")\n",
    "            END,\n",
    "            @@display_node_set += t;\n",
    "            hop = hop + 1;\n",
    "    END;\n",
    "    hop = hop - 1;\n",
    "    PRINT @@sum_total_dist;\n",
    "    PRINT hop;\n",
    "    IF print_stats THEN\n",
    "        tmp = {@@display_node_set};\n",
    "        PRINT @@display_edge_set,tmp;\n",
    "    END;\n",
    "ELSE\n",
    "    PRINT \"No viable path found.\";\n",
    "END;\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_edge(conn, source_vertex_id, target_vertex_id, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    \n",
    "    try:\n",
    "        result = conn.delEdges(vertex_type,source_vertex_id,edgeType,vertex_type,target_vertex_id)\n",
    "        \n",
    "        # Afficher le résultat pour comprendre sa structure\n",
    "        print(\"Result of delEdges:\", result)\n",
    "        \n",
    "        # Si le format attendu est différent, on peut adapter le traitement ici\n",
    "        if isinstance(result, list):  # Si le résultat est une liste\n",
    "            for r in result:\n",
    "                print(f\"Deleted edges: {r.get('deleted_edges')}, Edge Type: {r.get('e_type')}\")\n",
    "        elif isinstance(result, dict):  # Si c'est un dictionnaire\n",
    "            print(f\"Deleted edges: {result.get('deleted_edges')}, Edge Type: {result.get('e_type')}\")\n",
    "        else:\n",
    "            print(\"Unexpected result format\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except TypeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: string indices must be integers, not 'str'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43merase_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMadrid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSeville\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36merase_edge\u001b[1;34m(conn, source_vertex_id, target_vertex_id, vertex_type, edgeType)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m logs_modif_network\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelEdges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource_vertex_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43medgeType\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvertex_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_vertex_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Afficher le résultat pour comprendre sa structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult of delEdges:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphEdge.py:1083\u001b[0m, in \u001b[0;36mpyTigerGraphEdge.delEdges\u001b[1;34m(self, sourceVertexType, sourceVertexId, edgeType, targetVertexType, targetVertexId, where, limit, sort, timeout)\u001b[0m\n\u001b[0;32m   1081\u001b[0m ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m-> 1083\u001b[0m     ret[r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeleted_edges\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m   1086\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ret))\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "erase_edge(conn,\"Madrid\",\"Seville\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GraphName': 'VWG',\n",
       " 'VertexTypes': [{'Config': {'STATS': 'OUTDEGREE_BY_EDGETYPE'},\n",
       "   'Attributes': [{'AttributeType': {'Name': 'FLOAT'},\n",
       "     'AttributeName': 'LoadCapacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'UnloadCapacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Stock'},\n",
       "    {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'Carga'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Capacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'latitude'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'longitude'}],\n",
       "   'PrimaryId': {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'id'},\n",
       "   'Name': 'Nodes'}],\n",
       " 'EdgeTypes': [{'IsDirected': False,\n",
       "   'ToVertexTypeName': 'Nodes',\n",
       "   'Config': {},\n",
       "   'Attributes': [{'AttributeType': {'Name': 'FLOAT'},\n",
       "     'AttributeName': 'Price'},\n",
       "    {'AttributeType': {'Name': 'STRING'}, 'AttributeName': 'Carga'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Capacity'},\n",
       "    {'AttributeType': {'Name': 'FLOAT'}, 'AttributeName': 'Daily_movement'}],\n",
       "   'FromVertexTypeName': 'Nodes',\n",
       "   'Name': 'distribute_to'}],\n",
       " 'UDTs': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.getSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret token 1: 395kqmln948q1cf8aspr7vnd479l216i\n"
     ]
    }
   ],
   "source": [
    "# # FREE CLUSTER EDGE\n",
    "hostName1 = \"https://506118e7a5824411b8cff3b579d8809b.i.tgcloud.io\"\n",
    "graphName1 = \"VWG1\"\n",
    "secret1 = \"4alh4gsrhiq5tlfb3tf5gro5srpa5e35\"\n",
    "userName1 = \"user_1\"\n",
    "password1 = \"A1z2e3r4*\"\n",
    "tigergraph_insights_map1 = \"https://tools.tgcloud.io/insights/app/gBQu5GpNcMMoNttUGH8t29/page/mwMTnWnUDwp1PgFEE5Vapi/widgetShare/veFZVtxZW1gpd4GXchssZ2?domain=506118e7a5824411b8cff3b579d8809b.i&orgName=jrlisdata-org-2024325&clusterid=95a180a6-6c60-4d83-94ff-5291bf665469&TigerGraphToken=2cf416dd-3e79-4428-a5c0-1a68168b608b\"\n",
    "graph1 = tg.TigerGraphConnection(host=hostName1, graphname=graphName1)\n",
    "\n",
    "authToken1 = graph1.getToken(secret1)\n",
    "authToken1 = authToken1[0]\n",
    "print(f\"secret token 1: {authToken1}\")\n",
    "conn1 = tg.TigerGraphConnection(host=hostName1, graphname=graphName1, username=userName1, password=password1, apiToken=authToken1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para las entregas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "parameter without a default follows parameter with a default (4240550438.py, line 332)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 332\u001b[1;36m\u001b[0m\n\u001b[1;33m    def make_daily_movements(conn=conn, input_file, column_origin, column_destination, column_transfert):\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m parameter without a default follows parameter with a default\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "atributes_nodes = {}\n",
    "atributes_edges = {}\n",
    "\n",
    "\n",
    "def path_taken(results):\n",
    "\n",
    "    \"\"\"\n",
    "    Determina y devuelve el orden de los nodos que conforman el camino tomado a partir de los resultados\n",
    "    de una consulta de ruta en el grafo (el algoritmo suele devolver los resultado en el desorden)\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrae el conjunto de aristas desde el diccionario de resultados.\n",
    "    edges = results[2]['@@display_edge_set']\n",
    "    \n",
    "    # Inicializa una variable para almacenar el nodo inicial.\n",
    "    node_initial = None\n",
    "    \n",
    "    # Crea un conjunto de identificadores de nodos a los que se dirige cada arista.\n",
    "    edge_to_ids = {edge['to_id'] for edge in edges}\n",
    "    \n",
    "    # Busca el nodo inicial (el nodo que no es el destino de ningún arista).\n",
    "    for edge in edges:\n",
    "        if edge['from_id'] not in edge_to_ids:\n",
    "            node_initial = edge['from_id']\n",
    "            break\n",
    "    \n",
    "    # Si no se encuentra un nodo inicial, lanza una excepción.\n",
    "    if node_initial is None:\n",
    "        raise ValueError(\"initial node cannot be determined.\")\n",
    "    \n",
    "    # Inicializa el nodo actual como el nodo inicial y crea una lista para el camino.\n",
    "    current_node = node_initial\n",
    "    path = [current_node]\n",
    "    \n",
    "    # Recorre los bordes para construir el camino desde el nodo inicial.\n",
    "    while True:\n",
    "        next_node = None\n",
    "        \n",
    "        # Busca el siguiente nodo en el camino basado en el nodo actual.\n",
    "        for edge in edges:\n",
    "            if edge['from_id'] == current_node:\n",
    "                next_node = edge['to_id']\n",
    "                break\n",
    "        \n",
    "        # Si no se encuentra el siguiente nodo, termina el bucle.\n",
    "        if next_node is None:\n",
    "            break\n",
    "        \n",
    "        # Añade el siguiente nodo al camino y actualiza el nodo actual.\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "    \n",
    "    # Crea un diccionario de nodos mapeando cada nodo a sí mismo.\n",
    "    nodes = {node_name: node_name for node_name in path}\n",
    "    \n",
    "    # Crea una lista ordenada de nodos basada en el camino encontrado.\n",
    "    ordered_nodes = [nodes[node_name] for node_name in path]\n",
    "    \n",
    "    # Devuelve la lista de nodos ordenados que representa el camino tomado.\n",
    "    return ordered_nodes\n",
    "\n",
    "\n",
    "def add_or_update_final_nodes_batch(nodes_batch_full, nodes_batch, charge):\n",
    "\n",
    "    \"\"\"\n",
    "    Actualiza o agrega nodos en el lote completo de nodos basado en las transferencias realizadas, ajustando el stock según la carga.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crea una lista de ciudades existentes a partir de los nodos en 'nodes_batch_full'.\n",
    "    existing_cities = [city for city, _ in nodes_batch_full]\n",
    "    \n",
    "    # Itera sobre cada nodo en el 'nodes_batch' para actualizar o agregar nodos en 'nodes_batch_full'.\n",
    "    for node, _ in nodes_batch:\n",
    "        if node in existing_cities:\n",
    "            # Si el nodo ya existe en 'nodes_batch_full', busca el índice del nodo para actualizar el stock.\n",
    "            for i, (city, stock) in enumerate(nodes_batch_full):\n",
    "                if city == node:\n",
    "                    # Si es el primer nodo, decrementa el stock por 'charge'.\n",
    "                    # De lo contrario, incrementa el stock por 'charge'.\n",
    "                    if i == 0:\n",
    "                        nodes_batch_full[i] = (city, {\"Stock\": nodes_batch_full[i][1][\"Stock\"] - charge})\n",
    "                    else:\n",
    "                        nodes_batch_full[i] = (city, {\"Stock\": nodes_batch_full[i][1][\"Stock\"] + charge})\n",
    "            continue\n",
    "        else:\n",
    "            # Si el nodo no está en 'nodes_batch_full', agrega nuevas entradas para los nodos de 'nodes_batch'.\n",
    "            nodes_batch_full.extend([\n",
    "                (nodes_batch[0][0], {\"Stock\": nodes_batch[0][1][\"Stock\"]}),\n",
    "                (nodes_batch[1][0], {\"Stock\": nodes_batch[1][1][\"Stock\"]})\n",
    "            ])\n",
    "            # Retorna 'nodes_batch_full' después de agregar las nuevas entradas.\n",
    "            return nodes_batch_full\n",
    "    \n",
    "    # Retorna 'nodes_batch_full' después de actualizar o agregar los nodos.\n",
    "    return nodes_batch_full\n",
    "\n",
    "def add_or_update_final_edges_batch(edges_batch_full, edges_batch, charge):\n",
    "\n",
    "    \"\"\"\n",
    "    Actualiza o agrega aristas en el lote completo de aristas basado en las transferencias realizadas, ajustando el movimiento diario según la carga.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Crea una lista de bordes existentes a partir de 'edges_batch_full', considerando ambos sentidos.\n",
    "    existing_edges = [(edge[0], edge[1]) for edge in edges_batch_full]\n",
    "    \n",
    "    # Itera sobre cada borde en 'edges_batch' para actualizar o agregar bordes en 'edges_batch_full'.\n",
    "    for edge in edges_batch:\n",
    "        # Verifica si el borde ya existe en 'edges_batch_full' considerando ambos sentidos.\n",
    "        if (edge[0], edge[1]) in existing_edges or (edge[1], edge[0]) in existing_edges:\n",
    "            # Si el borde ya existe, busca el índice del borde para actualizar el movimiento diario.\n",
    "            for i, (origin, destination, movement) in enumerate(edges_batch_full):\n",
    "                # Compara ambos sentidos del borde para encontrar el índice correcto.\n",
    "                if (origin, destination) == (edge[0], edge[1]) or (origin, destination) == (edge[1], edge[0]):\n",
    "                    # Actualiza el movimiento diario del borde sumando 'charge'.\n",
    "                    edges_batch_full[i] = (origin, destination, {\"Daily_movement\": edges_batch_full[i][2][\"Daily_movement\"] + charge})\n",
    "            continue\n",
    "        else:\n",
    "            # Si el borde no existe, agrega una nueva entrada en 'edges_batch_full'.\n",
    "            edges_batch_full.append((edge[0], edge[1], {\"Daily_movement\": edge[2][\"Daily_movement\"]}))\n",
    "    \n",
    "    # Retorna 'edges_batch_full' después de actualizar o agregar los bordes.\n",
    "    return edges_batch_full\n",
    "\n",
    "\n",
    "def actualize_graph_carga(conn=conn):\n",
    "    \"\"\"\n",
    "    Actualiza el estado de carga de los nodos y aristas en el grafo, clasifica cada uno según su nivel de carga y registra los estados detectados\n",
    "    Esto se hace con el objetivo de camibar los colores en funccion de la carga en el mapa de Tigergraph, creo que no es necesario para prod.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtiene la lista de nodos y aristas desde la conexión proporcionada.\n",
    "    nodes = conn.getVertices(\"Nodes\")\n",
    "    aristas = conn.getEdgesByType(edgeType=\"distribute_to\")\n",
    "\n",
    "    # Inicializa listas para almacenar nodos y aristas con datos escalados.\n",
    "    nodes_batch_scale = []\n",
    "    aristas_batch_scale = []\n",
    "    \n",
    "    # Inicializa una variable para registrar los estados de nodos y aristas.\n",
    "    logs_state_nodes_and_arristas = \"\"\n",
    "\n",
    "    # Contadores para los diferentes niveles de carga de nodos y aristas.\n",
    "    node_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0}\n",
    "    arista_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0}\n",
    "\n",
    "    # Itera sobre cada nodo para determinar su nivel de carga.\n",
    "    for node in nodes:\n",
    "        node_capacidad = node[\"attributes\"][\"Capacity\"]\n",
    "        node_stock = node[\"attributes\"][\"Stock\"]\n",
    "        \n",
    "        # Determina el nivel de carga del nodo basado en su capacidad y stock.\n",
    "        if node_capacidad == 0:\n",
    "            nivel_stock = \"No definido\"\n",
    "        else:\n",
    "            if (node_stock / node_capacidad) > 1:\n",
    "                nivel_stock = \"Sobrecargado\"\n",
    "                node_counts[\"Sobrecargado\"] += 1\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ {node['v_id']} es {nivel_stock} \\n\"\n",
    "            elif 0.7 < (node_stock / node_capacidad) <= 1:\n",
    "                nivel_stock = \"Optimal\"\n",
    "                node_counts[\"Optimal\"] += 1\n",
    "            else:\n",
    "                nivel_stock = \"Subcargado\"\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ {node['v_id']} es {nivel_stock} \\n\"\n",
    "                node_counts[\"Subcargado\"] += 1\n",
    "\n",
    "        # Añade el nodo con su nivel de carga a la lista de nodos a escalar.\n",
    "        nodes_batch_scale.append((node[\"v_id\"], {\"Carga\": nivel_stock}))\n",
    "\n",
    "    # Itera sobre cada arista para determinar su nivel de carga.\n",
    "    for arista in aristas:\n",
    "        capacity_edge = arista[\"attributes\"][\"Capacity\"]\n",
    "        daily_movement = arista[\"attributes\"][\"Daily_movement\"]\n",
    "        \n",
    "        # Determina el nivel de carga de la arista basado en su capacidad y movimiento diario.\n",
    "        if capacity_edge == 0:\n",
    "            nivel_arista = \"No definido\"\n",
    "        else:\n",
    "            if (daily_movement / capacity_edge) > 1:\n",
    "                nivel_arista = \"Sobrecargado\"\n",
    "                arista_counts[\"Sobrecargado\"] += 1\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ la arista {arista['from_id']} hasta {arista['to_id']} es {nivel_arista} \\n\"\n",
    "            elif 0.7 < (daily_movement / capacity_edge) <= 1:\n",
    "                nivel_arista = \"Optimal\"\n",
    "                arista_counts[\"Optimal\"] += 1\n",
    "            else:\n",
    "                nivel_arista = \"Subcargado\"\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ la arista {arista['from_id']} hasta {arista['to_id']} es {nivel_arista} \\n\"\n",
    "                arista_counts[\"Subcargado\"] += 1\n",
    "\n",
    "        # Añade la arista con su nivel de carga a la lista de aristas a escalar.\n",
    "        aristas_batch_scale.append((arista[\"from_id\"], arista[\"to_id\"], {\"Carga\": nivel_arista}))\n",
    "\n",
    "    # Actualiza los nodos en la base de datos con los datos escalados.\n",
    "    conn.upsertVertices(\"Nodes\", nodes_batch_scale)\n",
    "    \n",
    "    # Actualiza las aristas en la base de datos con los datos escalados.\n",
    "    conn.upsertEdges(sourceVertexType=\"Nodes\", targetVertexType=\"Nodes\", edgeType=\"distribute_to\", edges=aristas_batch_scale)\n",
    "\n",
    "    # Si no se registraron cambios, indica que todos los nodos y aristas están en un estado óptimo.\n",
    "    if logs_state_nodes_and_arristas == \"\":\n",
    "        logs_state_nodes_and_arristas = \"Todos los nodos y aristas están en un estado óptimo.\"\n",
    "\n",
    "    # Crea un diccionario con los registros de estado y los conteos de nodos y aristas.\n",
    "    results = {\n",
    "        \"logs\": logs_state_nodes_and_arristas,\n",
    "        \"node_counts\": node_counts,\n",
    "        \"arista_counts\": arista_counts\n",
    "    }\n",
    "\n",
    "    # Retorna el diccionario con los resultados.\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main transfer function\n",
    "\n",
    "def transfert_nodes_and_edges(node_initial, node_final, charge, weight_attribute=\"Capacity\", nodetype=\"Nodes\", edgetype=\"distribute_to\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Realiza la transferencia de una carga desde un nodo inicial a un nodo final siguiendo el camino óptimo encontrado por el algoritmo A*\n",
    "    actualizando los stocks de los nodos y el movimiento diario de las aristas involucradas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lanza la función A* para encontrar el camino óptimo entre node_initial y node_final\n",
    "    results_astar = conn.runInstalledQuery(\"tg_astar\", params={\n",
    "        \"source_vertex\": node_final, \"source_vertex.type\": nodetype,\n",
    "        \"target_vertex\": node_initial, \"target_vertex.type\": nodetype,\n",
    "        \"e_type_set\": edgetype, \"weight_type\": \"FLOAT\",\n",
    "        \"latitude\": \"latitude\", \"longitude\": \"longitude\",\n",
    "        \"weight_attribute\": weight_attribute,\n",
    "        \"print_stats\": \"True\"\n",
    "    })\n",
    "    \n",
    "    # Obtiene el orden de los nodos en el camino encontrado\n",
    "    order_taken = path_taken(results_astar)\n",
    "    \n",
    "    # Obtiene el conjunto de aristas involucradas en el camino\n",
    "    edge_set = results_astar[2]['@@display_edge_set']\n",
    "    \n",
    "    # Reorganiza las aristas de acuerdo al orden de los nodos en el camino\n",
    "    reordered_edges_set = []\n",
    "    for location in order_taken:\n",
    "        for edge in edge_set:\n",
    "            if edge['from_id'] == location:\n",
    "                reordered_edges_set.append(edge)\n",
    "\n",
    "    # Obtiene el conjunto de nodos involucrados en el camino\n",
    "    node_set = results_astar[2]['tmp']\n",
    "    \n",
    "    # Reorganiza los nodos de acuerdo al orden en el camino\n",
    "    reordered_nodes_set = []\n",
    "    for location in order_taken:\n",
    "        for item in node_set:\n",
    "            if item['v_id'] == location:\n",
    "                reordered_nodes_set.append(item)\n",
    "\n",
    "    try:\n",
    "        # Recupera los atributos de los nodos inicial y final\n",
    "        node_i_stock = reordered_nodes_set[0][\"attributes\"][\"Stock\"]\n",
    "        node_f_stock = reordered_nodes_set[-1][\"attributes\"][\"Stock\"]\n",
    "        node_i_unload_capacity = reordered_nodes_set[0][\"attributes\"][\"UnloadCapacity\"]\n",
    "        node_f_load_capacity = reordered_nodes_set[-1][\"attributes\"][\"LoadCapacity\"]\n",
    "\n",
    "        # Verifica si hay suficiente inventario en el nodo inicial\n",
    "        if charge > node_i_stock:\n",
    "            error_message = f\"{node_initial} hasta {node_final}: No hay suficiente inventario ({charge} > {node_i_stock})\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "        # Verifica si hay suficiente capacidad de carga en el nodo final\n",
    "        if charge > node_f_load_capacity:\n",
    "            error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de carga en el almacén de recepción (charge: {charge} > LoadCapacity: {node_f_load_capacity})\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "        # Verifica si hay suficiente capacidad de descarga en el nodo inicial\n",
    "        if charge > node_i_unload_capacity:\n",
    "            error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de descarga desde el almacén de salida (charge: {charge} > UnloadCapacity: {node_i_unload_capacity})\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "        # Recupera la capacidad de la arista y el movimiento diario actual\n",
    "        capacity_edge = reordered_edges_set[0][\"attributes\"][\"Capacity\"]\n",
    "        daily_movement = reordered_edges_set[0][\"attributes\"][\"Daily_movement\"]\n",
    "\n",
    "        # Verifica si la arista puede soportar el movimiento adicional de carga\n",
    "        if daily_movement + charge > capacity_edge:\n",
    "            error_message = f\"Enviar {charge} palets desde {node_initial} hasta {node_final}: Capacidad de la arista insuficiente ({daily_movement + charge} > {capacity_edge})\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "        # Actualiza los inventarios de los nodos inicial y final\n",
    "        nodes_batch = [\n",
    "            (node_initial, {\"Stock\": node_i_stock - charge}),\n",
    "            (node_final, {\"Stock\": node_f_stock + charge})\n",
    "        ]\n",
    "\n",
    "        # Actualiza el movimiento diario en las aristas recorridas y acumula el costo\n",
    "        edges_batch = []\n",
    "        for element in reordered_edges_set:\n",
    "            element['attributes']['Daily_movement'] += charge\n",
    "            edges_batch.append((element[\"from_id\"], element[\"to_id\"], {\"Daily_movement\": element[\"attributes\"][\"Daily_movement\"]}))\n",
    "            palets_cost = 5\n",
    "            palets_cost += element[\"attributes\"][\"Price\"]\n",
    "\n",
    "        # Añade los nodos inicial y final a los conjuntos de nodos únicos\n",
    "        unique_origin_nodes.add(node_initial)\n",
    "        unique_destination_nodes.add(node_final)\n",
    "        \n",
    "        # Incrementa el conteo de operaciones y el número total de palets\n",
    "        operation_count = 100\n",
    "        operation_count += 1\n",
    "        palets_number = 22\n",
    "        palets_number += charge\n",
    "\n",
    "        return nodes_batch , edges_batch\n",
    "\n",
    "    # Manejo de excepciones en caso de que ocurra algún error durante el proceso\n",
    "    except Exception as e:\n",
    "        log_list += f\"Error durante la transferencia de {node_initial} a {node_final}: {str(e)}\\n\"\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error en la transferancia: {e}\\n\"\n",
    "        log_list += error_message\n",
    "        return error_message\n",
    "\n",
    "\n",
    "# Thread safety\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def make_daily_movements(conn=conn, input_file, column_origin, column_destination, column_transfert):\n",
    "\n",
    "    \"\"\"\n",
    "    Funccion principal : actualiza los logs, hace las transferencias (entregas) en paralelo con threads, y actualiza el grafo\n",
    "    \"\"\"\n",
    "\n",
    "    # Variables globales para rastrear estadísticas y almacenar datos.\n",
    "    global operation_count, palets_number, palets_cost, unique_origin_nodes, unique_destination_nodes, log_list, nodes_batch_full, edges_batch_full, df\n",
    "    operation_count = 0  # Conteo de operaciones realizadas.\n",
    "    palets_number = 0  # Número total de palets transferidos.\n",
    "    palets_cost = 0  # Costo total asociado a los palets transferidos.\n",
    "    unique_origin_nodes = set()  # Conjunto de nodos de origen únicos.\n",
    "    unique_destination_nodes = set()  # Conjunto de nodos de destino únicos.\n",
    "    log_list = \" \"  # Registro de errores o mensajes importantes.\n",
    "    nodes_batch_full = []  # Lista para almacenar nodos actualizados.\n",
    "    edges_batch_full = []  # Lista para almacenar aristas actualizadas.\n",
    "\n",
    "    # Lee el archivo CSV y carga los datos en un DataFrame.\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Función para manejar la transferencia de volumen entre nodos.\n",
    "    def perform_transfer(row):\n",
    "        global log_list\n",
    "        try:\n",
    "            # Obtiene el origen, destino y volumen desde la fila del DataFrame.\n",
    "            origin = str(row[column_origin])\n",
    "            destination = str(row[column_destination])\n",
    "            volume = float(row[column_transfert])\n",
    "            \n",
    "            # Llama a la función para obtener nodos y aristas actualizados.\n",
    "            nodes_batch, edges_batch = transfert_nodes_and_edges(origin, destination, volume)\n",
    "            \n",
    "            # Usa un lock para garantizar que las actualizaciones a las listas sean seguras en un entorno multihilo.\n",
    "            with lock:\n",
    "                # Actualiza o agrega nodos y aristas a las listas correspondientes.\n",
    "                add_or_update_final_nodes_batch(nodes_batch_full, nodes_batch, volume)\n",
    "                add_or_update_final_edges_batch(edges_batch_full, edges_batch, volume)\n",
    "            \n",
    "            # Actualiza las métricas globales.\n",
    "            global operation_count, palets_number, palets_cost\n",
    "            operation_count += 1  # Incrementa el conteo de operaciones.\n",
    "            palets_number += volume  # Incrementa el número total de palets.\n",
    "            palets_cost += volume  # Incrementa el costo total (asumiendo un costo por palet definido como some_cost_per_palet).\n",
    "            unique_origin_nodes.add(origin)  # Agrega el nodo de origen al conjunto de nodos únicos.\n",
    "            unique_destination_nodes.add(destination)  # Agrega el nodo de destino al conjunto de nodos únicos.\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Registra un mensaje de error si ocurre una excepción.\n",
    "            error_message = f\"{origin} hasta {destination}, no camino posible.\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "    # Usa un ThreadPoolExecutor para procesar las transferencias en paralelo.\n",
    "    with ThreadPoolExecutor(max_workers=1000) as executor:\n",
    "        # Crea y envía tareas para cada fila del DataFrame.\n",
    "        futures = [executor.submit(perform_transfer, row) for index, row in df.iterrows()]\n",
    "        # Espera a que todas las tareas se completen.\n",
    "        for future in as_completed(futures):\n",
    "            pass  # Se pueden manejar los resultados de las tareas aquí si es necesario.\n",
    "\n",
    "    # Actualiza los nodos y aristas en la base de datos.\n",
    "    conn.upsertVertices(\"Nodes\", nodes_batch_full)\n",
    "    nodetype = \"Nodes\"\n",
    "    edgetype = \"distribute_to\"\n",
    "    conn.upsertEdges(sourceVertexType=nodetype, targetVertexType=nodetype, edgeType=edgetype, edges=edges_batch_full)\n",
    "    \n",
    "    # Llama a la función para actualizar el gráfico con el estado actual y obtiene el estado.\n",
    "    state_log = actualize_graph_carga(conn)\n",
    "\n",
    "    # Si no hubo errores, actualiza el mensaje de log para indicar éxito.\n",
    "    if log_list == \" \":\n",
    "        log_list = \"All operations were successful.\"\n",
    "\n",
    "    print(\"Número de operaciones realizadas:\", operation_count)\n",
    "    print(\"Número total de palets transferidos:\", palets_number)\n",
    "    print(\"Costo total asociado a los palets transferidos:\", palets_cost)\n",
    "    print(\"Conjunto de nodos de origen únicos:\", unique_origin_nodes)\n",
    "    print(\"Conjunto de nodos de destino únicos:\", unique_destination_nodes)\n",
    "    print(\"Registro de errores o mensajes importantes:\", log_list)\n",
    "    print(\"Lista de nodos actualizados:\", nodes_batch_full)\n",
    "    print(\"Lista de aristas actualizadas:\", edges_batch_full)\n",
    "\n",
    "\n",
    "    # Devuelve un conjunto de resultados incluyendo logs, un botón para recargar el mapa y KPIs actualizados (elementos gradio)\n",
    "    return (\n",
    "        log_list,\n",
    "        gr.Button(\"Recargar mapa\", elem_classes=\"otherbutton\", visible=True),\n",
    "        gr.Number(label=\"KPI : Numero de operaciones\", value=operation_count, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Numero de palets\", value=palets_number, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Numero de origen\", value=len(unique_origin_nodes), visible=True, elem_classes=\"dropdown\"),  \n",
    "        gr.Number(label=\"KPI : Destinaciones diferentes\", value=len(unique_destination_nodes), visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Number(label=\"KPI : Coste total\", value=palets_cost, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Coste por palet\", value=round(palets_cost / 1, 1), visible=True, elem_classes=\"dropdown\") \n",
    "    )\n",
    "\n",
    "\n",
    "# log_messages = make_daily_movements(r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\CSV_entrega\\df_1_move.csv\", \"CODE_Origin\", \"CODE_Destination\", \"Palets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de modificacion de red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui siempre utilizo conn1 y no conn porque se modificaba solo el segundo grafo, creo que se deberia parametrizar \"conn1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "global logs_modif_network\n",
    "logs_modif_network = \" \"\n",
    "\n",
    "\n",
    "\n",
    "# ADD NODES \n",
    "\n",
    "def add_node(id, attributes_dict=None, vertex_type=\"Nodes\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertVertex(vertex_type, id)\n",
    "        message =  f\"Nodo {id} añadido\\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        conn1.upsertVertex(vertex_type, id, attributes=attributes)\n",
    "        message =  f\"Nodo {id} añadido with attributos : {attributes} \\n\"\n",
    "        logs_modif_network +=message\n",
    "    return logs_modif_network\n",
    "\n",
    "def get_values_nodes(*args):\n",
    "    values = {key: val for key, val in zip(atributes_nodes.keys(), args)}\n",
    "    return values\n",
    "\n",
    "def get_values_edges(*args):\n",
    "    values = {key: val for key, val in zip(atributes_edges.keys(), args)}\n",
    "    return values[\"Capacity\"]\n",
    "\n",
    "\n",
    "def add_node_with_values(id, *args):\n",
    "    attributes = get_values_nodes(*args)\n",
    "    attributes_json = json.dumps(attributes)\n",
    "    actualize_graph_carga(conn)\n",
    "    return add_node(id, attributes_json)\n",
    "\n",
    "\n",
    "\n",
    "# ADD EDGES \n",
    "def add_new_edge(source_vertex_id, target_vertex_id, attributes_dict=None,source_vertex_type=\"Nodes\",target_vertex_type = \"Nodes\", edge_type=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertEdge(source_vertex_type, source_vertex_id, edge_type, target_vertex_type, target_vertex_id)\n",
    "        actualize_graph_carga(conn)\n",
    "        message =  f\"Arista de {source_vertex_id} hasta {target_vertex_id}  añadida\"\n",
    "        logs_modif_network += message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        actualize_graph_carga(conn)\n",
    "        conn1.upsertEdge(source_vertex_type, source_vertex_id, edge_type, target_vertex_type, target_vertex_id, attributes=attributes)\n",
    "        message =  f\"Arista de {source_vertex_id} hasta {target_vertex_id} añadida con los atributos siguientes: {attributes} \\n\"\n",
    "        logs_modif_network += message\n",
    "    return logs_modif_network\n",
    "\n",
    "def add_edge_with_values(source_vertex_id, target_vertex_id, *args):\n",
    "    attributes = get_values_edges(*args)\n",
    "    attributes_json = json.dumps(attributes)\n",
    "    return add_new_edge(source_vertex_id,target_vertex_id, attributes_json)\n",
    "\n",
    "# GET ATTRIBUTES\n",
    "attributes_nodes_list = []\n",
    "attributes_edges_list = []\n",
    "conex_nodes_list = conn1.getVertexAttrs('Nodes')\n",
    "conex_edges_list = conn1.getEdgeAttrs('distribute_to')\n",
    "\n",
    "for node in conex_nodes_list:\n",
    "    attributes_nodes_list.append(node[0])\n",
    "\n",
    "\n",
    "for edge in conex_edges_list:\n",
    "    attributes_edges_list.append(edge[0])\n",
    "\n",
    "\n",
    "#Remove carga atributes as it is here only for a cheat with colors of the tigergraph map. no need to change that\n",
    "if 'Carga' in attributes_nodes_list:\n",
    "    attributes_nodes_list.remove('Carga')\n",
    "\n",
    "if 'Carga' in attributes_edges_list:\n",
    "    attributes_edges_list.remove('Carga')\n",
    "\n",
    "\n",
    "# GET NODES NAMES\n",
    "list_name_nodes = []\n",
    "name_nodes_conn = conn1.getVertices(\"Nodes\")\n",
    "for i in range(0, len(name_nodes_conn)):\n",
    "    list_name_nodes.append(name_nodes_conn[i][\"v_id\"])\n",
    "\n",
    "def add_edge(source_vertex_id, target_vertex_id, attributes_dict=None, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertEdge(sourceVertexType=vertex_type,\n",
    "                        targetVertexType=vertex_type,\n",
    "                        edgeType=edgeType,\n",
    "                        sourceVertexId=source_vertex_id,\n",
    "                        targetVertexId=target_vertex_id)\n",
    "        message =  f\"Arista desde {source_vertex_id} hasta {target_vertex_id} añadido\\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        conn1.upsertEdge(sourceVertexType=vertex_type,\n",
    "                        targetVertexType=vertex_type,\n",
    "                        edgeType=edgeType,\n",
    "                        sourceVertexId=source_vertex_id,\n",
    "                        targetVertexId=target_vertex_id,\n",
    "                        attributes=attributes)\n",
    "        message =  f\"Arista {source_vertex_id} hasta {target_vertex_id} añadido con los atributos siguientes : {attributes} \\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "\n",
    "\n",
    "#Erase Data\n",
    "\n",
    "def erase_edge(source_vertex_id, target_vertex_id, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    result = conn1.delEdges(sourceVertexType=vertex_type,\n",
    "                           targetVertexType=vertex_type,\n",
    "                           edgeType=edgeType,\n",
    "                           sourceVertexId=source_vertex_id,\n",
    "                           targetVertexId=target_vertex_id)\n",
    "    \n",
    "    if result == {edgeType: 0}:\n",
    "        message = f\"Arista de {source_vertex_id} hacia {target_vertex_id} no existe o no ha podido borrarse\\n\"\n",
    "    else:\n",
    "        message = f\"Arista de  {source_vertex_id} hacia {target_vertex_id} borrada \\n\"\n",
    "    \n",
    "    logs_modif_network += message\n",
    "    return logs_modif_network\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def erase_node(id, vertex_type=\"Nodes\"):\n",
    "    global logs_modif_network\n",
    "    result = conn1.delVerticesById(vertexIds=id, vertexType=vertex_type)\n",
    "    \n",
    "    if result == 0:\n",
    "        message = f\"Nodo {id} no existe o no ha podido borrarse\\n\"\n",
    "    else:\n",
    "        message = f\"Nodo {id} borrado\\n\"\n",
    "    \n",
    "    logs_modif_network += message\n",
    "    return logs_modif_network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add_edge_with_values(\"Las Palmas de Gran Canaria\",\"Seville\",{\"Capacity\": 0, \"Daily_movement\": 0, \"Price\": 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TigerGraphException",
     "evalue": "('Processing attribute LoadCapacity failed, value cannot be converted to FLOAT', 'REST-30200')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTigerGraphException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     attributes_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(attributes)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add_node(\u001b[38;5;28mid\u001b[39m, attributes_json)\n\u001b[1;32m---> 24\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43madd_node_with_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoadCapacity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36madd_node_with_values\u001b[1;34m(id, *args)\u001b[0m\n\u001b[0;32m     19\u001b[0m attributes \u001b[38;5;241m=\u001b[39m get_values_nodes(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     20\u001b[0m attributes_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(attributes)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes_json\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36madd_node\u001b[1;34m(id, attributes_dict, vertex_type)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(attributes_dict)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mconn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsertVertex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     message \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNodo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m añadido with attributos : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattributes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m     logs_modif_network \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphVertex.py:229\u001b[0m, in \u001b[0;36mpyTigerGraphVertex.upsertVertex\u001b[1;34m(self, vertexType, vertexId, attributes)\u001b[0m\n\u001b[0;32m    226\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsertAttrs(attributes)\n\u001b[0;32m    227\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertices\u001b[39m\u001b[38;5;124m\"\u001b[39m: {vertexType: {vertexId: vals}}})\n\u001b[1;32m--> 229\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestppUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/graph/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccepted_vertices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    232\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ret))\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:454\u001b[0m, in \u001b[0;36mpyTigerGraphBase._post\u001b[1;34m(self, url, authMode, headers, data, resKey, skipCheck, params, jsonData)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    452\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m--> 454\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_req\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthMode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipCheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsonData\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjsonData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    457\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(res))\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:377\u001b[0m, in \u001b[0;36mpyTigerGraphBase._req\u001b[1;34m(self, method, url, authMode, headers, data, resKey, skipCheck, params, strictJson, jsonData, jsonResponse)\u001b[0m\n\u001b[0;32m    374\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skipCheck:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_errorCheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resKey:\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:257\u001b[0m, in \u001b[0;36mpyTigerGraphBase._errorCheck\u001b[1;34m(self, res)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if the JSON document returned by an endpoint has contains `error: true`. If so,\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    it raises an exception.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    TigerGraphException: if request returned with error, indicated in the returned JSON.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res \u001b[38;5;129;01mand\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# Endpoint might return string \"false\" rather than Boolean false\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TigerGraphException(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], (res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[1;31mTigerGraphException\u001b[0m: ('Processing attribute LoadCapacity failed, value cannot be converted to FLOAT', 'REST-30200')"
     ]
    }
   ],
   "source": [
    "atributes_nodes = {}\n",
    "atributes_edges = {}\n",
    "attributes_nodes_list = []\n",
    "attributes_edges_list = []\n",
    "conex_nodes_list = conn1.getVertexAttrs('Nodes')\n",
    "conex_edges_list = conn1.getEdgeAttrs('distribute_to')\n",
    "\n",
    "for node in conex_nodes_list:\n",
    "    attributes_nodes_list.append(node[0])\n",
    "\n",
    "atributes_nodes = {key: None for key in attributes_nodes_list}\n",
    "\n",
    "for edge in conex_edges_list:\n",
    "    attributes_edges_list.append(edge[0])\n",
    "atributes_edges = {key: None for key in attributes_edges_list}\n",
    "\n",
    "\n",
    "def add_node_with_values(id, *args):\n",
    "    attributes = get_values_nodes(*args)\n",
    "    attributes_json = json.dumps(attributes)\n",
    "    return add_node(id, attributes_json)\n",
    "\n",
    "node_id = \"TEST\"\n",
    "attributes_to_update = {\n",
    "    \"LoadCapacity\": 99\n",
    "}\n",
    "\n",
    "# Convert the attributes dictionary to a JSON string\n",
    "attributes_json = json.dumps(attributes_to_update)\n",
    "\n",
    "# Call the add_node function to upsert the node with the new attributes\n",
    "logs = add_node(node_id, attributes_json)\n",
    "\n",
    "# Print the logs to see the output\n",
    "print(logs)\n",
    "\n",
    "test = add_node_with_values(node_id,attributes_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nodo TEST añadido with attributos : {} \n",
      "Nodo TEST añadido with attributos : {} \n",
      "Nodo TEST añadido with attributos : {'LoadCapacity': 99} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the node ID and the new attribute you want to set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tab modif cargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_charge(conn, percentage, operation=\"increase\"):\n",
    "    \"\"\"\n",
    "    Ajusta el movimiento diario de todas las aristas del tipo 'distribute_to' en un porcentaje especificado.\n",
    "    Puede incrementar o decrementar el movimiento diario según la operación indicada.\n",
    "\n",
    "    Parámetros:\n",
    "    - conn: Conexión a la base de datos de TigerGraph.\n",
    "    - percentage: Porcentaje por el cual ajustar el movimiento diario de las aristas.\n",
    "    - operation: Tipo de ajuste a realizar ('increase' para incrementar, 'decrease' para decrementar).\n",
    "\n",
    "    Retorna:\n",
    "    - results: Diccionario con registros de logs y, si operation es 'decrease', conteo de aristas por nivel de carga.\n",
    "    - new_charges: Lista de tuplas con las aristas actualizadas y sus nuevos movimientos diarios.\n",
    "    \"\"\"\n",
    "    # Obtiene todas las aristas del tipo 'distribute_to'.\n",
    "    aristas = conn.getEdgesByType(edgeType=\"distribute_to\")\n",
    "    new_charges = []\n",
    "    \n",
    "    # Variables para logs\n",
    "    logs_state_aristas = \"\"\n",
    "    \n",
    "    # Variables para 'decrease' operation\n",
    "    arista_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0} if operation == \"decrease\" else None\n",
    "    \n",
    "    for arista in aristas:\n",
    "        current_charge = arista[\"attributes\"][\"Daily_movement\"]  # Movimiento diario actual.\n",
    "        capacity_edge = arista[\"attributes\"][\"Capacity\"]  # Capacidad de la arista.\n",
    "\n",
    "        if operation == \"increase\":\n",
    "            # Calcula el nuevo movimiento diario incrementándolo.\n",
    "            new_charge = current_charge * (1 + percentage / 100)\n",
    "        elif operation == \"decrease\":\n",
    "            # Calcula el nuevo movimiento diario decrementándolo.\n",
    "            new_charge = current_charge * (1 - percentage / 100)\n",
    "        else:\n",
    "            raise ValueError(\"La operación debe ser 'increase' o 'decrease'.\")\n",
    "\n",
    "        # Actualiza el atributo en la arista.\n",
    "        arista[\"attributes\"][\"Daily_movement\"] = new_charge\n",
    "\n",
    "        if operation == \"decrease\":\n",
    "            # Determina el nivel de carga de la arista después del ajuste.\n",
    "            if capacity_edge == 0:\n",
    "                nivel_arista = \"No definido\"\n",
    "            else:\n",
    "                ratio = new_charge / capacity_edge\n",
    "                if ratio > 1:\n",
    "                    nivel_arista = \"Sobrecargado\"\n",
    "                    arista_counts[\"Sobrecargado\"] += 1\n",
    "                    logs_state_aristas += (\n",
    "                        f\"⚠️ La arista {arista['from_id']} hasta {arista['to_id']} es \"\n",
    "                        f\"{nivel_arista} con un movimiento diario de {new_charge}\\n\"\n",
    "                    )\n",
    "                elif 0.7 < ratio <= 1:\n",
    "                    nivel_arista = \"Optimal\"\n",
    "                    arista_counts[\"Optimal\"] += 1\n",
    "                else:\n",
    "                    nivel_arista = \"Subcargado\"\n",
    "                    arista_counts[\"Subcargado\"] += 1\n",
    "            \n",
    "            # Agrega la arista actualizada con su nuevo movimiento diario y nivel de carga a la lista.\n",
    "            new_charges.append((\n",
    "                arista[\"from_id\"],\n",
    "                arista[\"to_id\"],\n",
    "                {\"Daily_movement\": new_charge, \"Carga\": nivel_arista}\n",
    "            ))\n",
    "        else:\n",
    "            # Para 'increase', solo actualiza el movimiento diario.\n",
    "            new_charges.append((\n",
    "                arista[\"from_id\"],\n",
    "                arista[\"to_id\"],\n",
    "                {\"Daily_movement\": new_charge}\n",
    "            ))\n",
    "            # Añade un log para cada incremento.\n",
    "            logs_state_aristas += (\n",
    "                f\"✅ La arista {arista['from_id']} hasta {arista['to_id']} ha sido incrementada a {new_charge}\\n\"\n",
    "            )\n",
    "    \n",
    "    if operation == \"decrease\":\n",
    "        # Si no se registraron aristas sobrecargadas, se añade un mensaje indicando que ninguna lo está.\n",
    "        if logs_state_aristas == \"\":\n",
    "            logs_state_aristas = \"Ninguna arista esta sobrecargado.\\n\"\n",
    "        \n",
    "        # Crea un diccionario con los logs y el conteo de aristas por nivel de carga.\n",
    "        results = {\n",
    "            \"logs\": logs_state_aristas,\n",
    "            \"arista_counts\": arista_counts,\n",
    "        }\n",
    "    else:\n",
    "        # Para 'increase', crea un diccionario solo con los logs.\n",
    "        results = {\n",
    "            \"logs\": logs_state_aristas\n",
    "        }\n",
    "    \n",
    "    return results, new_charges\n",
    "\n",
    "def update_batch_adjust(conn, percentage, operation=\"increase\"):\n",
    "    \"\"\"\n",
    "    Actualiza en lote las aristas incrementando o decrementando su movimiento diario en un porcentaje especificado.\n",
    "\n",
    "    Parámetros:\n",
    "    - conn: Conexión a la base de datos de TigerGraph.\n",
    "    - percentage: Porcentaje por el cual ajustar el movimiento diario de las aristas.\n",
    "    - operation: Tipo de ajuste a realizar ('increase' para incrementar, 'decrease' para decrementar).\n",
    "\n",
    "    Retorna:\n",
    "    - Si operation es 'increase':\n",
    "        - results: Diccionario con registros de logs.\n",
    "    - Si operation es 'decrease':\n",
    "        - results: Diccionario con registros de logs y conteo de aristas por nivel de carga.\n",
    "    \"\"\"\n",
    "    if operation not in [\"increase\", \"decrease\"]:\n",
    "        raise ValueError(\"La operación debe ser 'increase' o 'decrease'.\")\n",
    "\n",
    "    # Obtiene los logs y las aristas actualizadas con el ajuste aplicado.\n",
    "    results, batch = adjust_charge(conn, percentage, operation=operation)\n",
    "    \n",
    "    # Actualiza las aristas en la base de datos con los nuevos movimientos diarios.\n",
    "    conn.upsertEdges(\n",
    "        sourceVertexType=\"Nodes\",\n",
    "        targetVertexType=\"Nodes\",\n",
    "        edgeType=\"distribute_to\",\n",
    "        edges=batch\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# test = update_batch_adjust(conn,10,\"decrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_path(input_file):\n",
    "    df = pd.read_csv(input_file.name)\n",
    "    target_column = sorted(list(df.columns))\n",
    "    return  gr.Dropdown(choices = target_column) , gr.Dropdown(choices = target_column) , gr.Dropdown(choices = target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_edge = \"distribute_to\"\n",
    "name_node = \"Nodes\"\n",
    "\n",
    "\n",
    "#######RED ORIGINAL######\n",
    "#Hay que poner en funccion aqui. \n",
    "numEdges = conn.getEdgeCount(name_edge)\n",
    "numNodes = conn.getVertexCount(name_node)\n",
    "edge = conn.getEdgeStats(name_edge)\n",
    "avg_capacity = round(edge[name_edge]['Capacity']['AVG'],1)\n",
    "\n",
    "edges = conn.getEdgesByType(name_edge)\n",
    "total_capacity_edges = 0\n",
    "for warehouse in edges :\n",
    "    capacity_edges = warehouse[\"attributes\"][\"Capacity\"]     \n",
    "    total_capacity_edges += round(capacity_edges,1)\n",
    "\n",
    "almacenes = conn.getVertices(name_node)\n",
    "total_capacity_warehouse = 0\n",
    "for warehouse in almacenes :\n",
    "    capacity_edges = warehouse[\"attributes\"][\"Capacity\"]      \n",
    "    total_capacity_warehouse += round(capacity_edges,1)\n",
    "    total_capacity_warehouse = round(total_capacity_warehouse,1)\n",
    "\n",
    "almacen = conn.getVertices(name_node)\n",
    "total_stock = 0\n",
    "for warehouse in almacen :\n",
    "    stock = warehouse[\"attributes\"][\"Stock\"]     \n",
    "    total_stock += stock\n",
    "    total_stock = round(total_stock,0)\n",
    "\n",
    "capacity_per_edge = round(total_capacity_warehouse/numEdges,1)\n",
    "\n",
    "\n",
    "#######RED SIMULADA######\n",
    "import concurrent.futures\n",
    "\n",
    "def get_num_edges(conn, edge_name):\n",
    "    return conn.getEdgeCount(edge_name)\n",
    "\n",
    "def get_num_nodes(conn, node_name):\n",
    "    return conn.getVertexCount(node_name)\n",
    "\n",
    "def get_avg_capacity(conn, edge_name):\n",
    "    edges = conn.getEdgeStats(edge_name)\n",
    "    avg_capacity = edges[edge_name]['Capacity']['AVG']\n",
    "    return round(avg_capacity/2, 1)\n",
    "\n",
    "def get_total_capacity_edges(conn, edge_name):\n",
    "    edges = conn.getEdgesByType(edge_name)\n",
    "    total_capacity = 0\n",
    "    for edge in edges:\n",
    "        total_capacity += edge[\"attributes\"][\"Capacity\"]\n",
    "    return total_capacity /2\n",
    "\n",
    "def get_total_capacity_nodes(conn, node_name):\n",
    "    warehouses = conn.getVertices(node_name)\n",
    "    total_capacity = 0\n",
    "    for warehouse in warehouses:\n",
    "        total_capacity += warehouse[\"attributes\"][\"Capacity\"]\n",
    "    return round(total_capacity, 1)\n",
    "\n",
    "def get_total_stock(conn, node_name):\n",
    "    warehouses = conn.getVertices(node_name)\n",
    "    total_stock = 0\n",
    "    for warehouse in warehouses:\n",
    "        total_stock += warehouse[\"attributes\"][\"Stock\"]\n",
    "    return round(total_stock, 0)\n",
    "\n",
    "def get_capacity_per_edge(total_capacity_warehouses, num_edges):\n",
    "    if num_edges == 0:\n",
    "        return 0\n",
    "    return round(total_capacity_warehouses / num_edges, 0)\n",
    "\n",
    "def get_KPI(conn, edge_name, node_name):\n",
    "     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            \"num_edges\": executor.submit(get_num_edges, conn, edge_name),\n",
    "            \"num_nodes\": executor.submit(get_num_nodes, conn, node_name),\n",
    "            \"avg_capacity\": executor.submit(get_avg_capacity, conn, edge_name),\n",
    "            \"total_capacity_edges\": executor.submit(get_total_capacity_edges, conn, edge_name),\n",
    "            \"total_capacity_warehouses\": executor.submit(get_total_capacity_nodes, conn, node_name),\n",
    "            \"total_stock\": executor.submit(get_total_stock, conn, node_name)\n",
    "        }\n",
    "\n",
    "        results = {name: future.result() for name, future in futures.items()}\n",
    "        results[\"capacity_per_edge\"] = get_capacity_per_edge(results[\"total_capacity_warehouses\"], results[\"num_edges\"])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar Algoritmos A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE GRAPH VWG = Nombre del grafo (se necesita instalar por cada grafo y no cada cluster ! )\n",
    "#Create Query \"Nombre de la query\"\n",
    "\n",
    "result = conn.gsql(\"\"\"\n",
    "USE GRAPH VWG\n",
    "CREATE QUERY tg_astar (VERTEX source_vertex, VERTEX target_vertex, SET<STRING> e_type_set,\n",
    "STRING weight_type, STRING latitude, STRING longitude,\n",
    "STRING weight_attribute, BOOL print_stats = False) SYNTAX V1 {\n",
    "\n",
    "TYPEDEF TUPLE<FLOAT dist, VERTEX v> pathTuple;    # <shotest distance, parent node>\n",
    "HeapAccum<pathTuple>(1, dist ASC) @@find_min_v_heap;  # retain 1 shortest path\n",
    "HeapAccum<pathTuple>(1, dist ASC) @min_dist_heap;\n",
    "OrAccum @or_visited, @@or_valid_path_exists;\n",
    "ListAccum<VERTEX> @@tmp_list;  # the optimal node\n",
    "SumAccum<FLOAT> @@sum_total_dist;  # the shortest distance\n",
    "SetAccum<EDGE> @@display_edge_set;\n",
    "SetAccum<VERTEX> @@display_node_set;\n",
    "INT hop;\n",
    "FLOAT x1,y1;\n",
    "\n",
    "# Check weight_type parameter\n",
    "IF weight_type NOT IN (\"INT\", \"FLOAT\", \"DOUBLE\") THEN\n",
    "    PRINT \"weight_type must be INT, FLOAT, or DOUBLE\" AS errMsg;\n",
    "    RETURN;\n",
    "END;\n",
    "\n",
    "# record target latitude and longitude\n",
    "Tgt = {target_vertex};\n",
    "Tgt = SELECT s\n",
    "      FROM Tgt:s\n",
    "      POST-ACCUM x1 = s.getAttr(latitude,\"FLOAT\"),\n",
    "\t         y1 = s.getAttr(longitude,\"FLOAT\");\n",
    "\n",
    "Start = {source_vertex};   # the optimal node\n",
    "Opt = {source_vertex};    # all of the optimal nodes\n",
    "\n",
    "Start = SELECT s\n",
    "        FROM Start:s\n",
    "        ACCUM s.@or_visited = True,\n",
    "              s.@min_dist_heap = pathTuple(0,s);\n",
    "\n",
    "# run aster to find shortest distance greedily\n",
    "WHILE Opt.size() > 0 DO\n",
    "    # find the node with shortest distance\n",
    "    Opt = SELECT t\n",
    "          FROM Opt:s-(e_type_set:e)-> :t\n",
    "          WHERE t.@or_visited == False\n",
    "\t  ACCUM\n",
    "              # we use Haversine formula as the heuristic function here\n",
    "              CASE weight_type WHEN \"INT\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"INT\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              WHEN \"FLOAT\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"FLOAT\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              WHEN \"DOUBLE\" THEN\n",
    "                  t.@min_dist_heap += pathTuple(s.@min_dist_heap.top().dist + e.getAttr(weight_attribute, \"DOUBLE\")\n",
    "\t\t  +  tg_GetDistance(t.getAttr(latitude,\"FLOAT\"),t.getAttr(longitude,\"FLOAT\"),x1,y1),s)\n",
    "              END;\n",
    "\n",
    "    Opt = SELECT t\n",
    "          FROM Start:s-(e_type_set:e)-> :t\n",
    "          WHERE t.@or_visited == False\n",
    "          POST-ACCUM @@find_min_v_heap += pathTuple(t.@min_dist_heap.top().dist,t);\n",
    "\n",
    "    @@tmp_list.clear();\n",
    "    IF @@find_min_v_heap.size() > 0 THEN\n",
    "        @@tmp_list += @@find_min_v_heap.pop().v;\n",
    "    END;\n",
    "\n",
    "    Opt = {@@tmp_list};\n",
    "    Start = Opt UNION Start;\n",
    "    Opt = SELECT t\n",
    "          FROM Opt:t\n",
    "\t  POST-ACCUM\n",
    "\t      t.@or_visited += True;\n",
    "              # Determine if it is the target point and terminate the loop if it is\n",
    "              IF @@tmp_list.get(0) == target_vertex THEN\n",
    "                  BREAK;\n",
    "              END;\n",
    "\n",
    "END;\n",
    "# The test is whether there is a path between two points\n",
    "Start = {target_vertex};\n",
    "Start = SELECT s\n",
    "        FROM Start:s\n",
    "        POST-ACCUM @@or_valid_path_exists += s.@min_dist_heap.size() > 0,\n",
    "                   @@display_node_set += s;\n",
    "\n",
    "IF @@or_valid_path_exists THEN\n",
    "    # find path\n",
    "    WHILE Start.size() > 0 DO\n",
    "        Start =\n",
    "\tSELECT t\n",
    "\tFROM Start:s-(e_type_set:e)-> :t\n",
    "        WHERE t == s.@min_dist_heap.top().v\n",
    "        ACCUM\n",
    "\t    @@display_edge_set += e,\n",
    "            CASE weight_type WHEN \"INT\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"INT\")\n",
    "            WHEN \"FLOAT\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"FLOAT\")\n",
    "            WHEN \"DOUBLE\" THEN\n",
    "                @@sum_total_dist += e.getAttr(weight_attribute, \"DOUBLE\")\n",
    "            END,\n",
    "            @@display_node_set += t;\n",
    "            hop = hop + 1;\n",
    "    END;\n",
    "    hop = hop - 1;\n",
    "    PRINT @@sum_total_dist;\n",
    "    PRINT hop;\n",
    "    IF print_stats THEN\n",
    "        tmp = {@@display_node_set};\n",
    "        PRINT @@display_edge_set,tmp;\n",
    "    END;\n",
    "ELSE\n",
    "    PRINT \"No viable path found.\";\n",
    "END;\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definida la query, se necesita instalar con : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conn.gsql(\"\"\"\n",
    "USE GRAPH VWG           #Nombre del grafo\n",
    "INSTALL QUERY tg_astar  #Nombre del algoritmo\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deberias tener un resultado del estilo :\n",
    " \n",
    "Using graph \\'VWG\\'\\nStart installing queries, about 1 minute ...\\ntg_astar query: curl -X GET \\'https://127.0.0.1:9000/query/VWG/tg_astar?source_vertex=VALUE&source_vertex.type=VERTEX_TYPE&target_vertex=VALUE&target_vertex.type=VERTEX_TYPE&e_type_set=VALUE&weight_type=VALUE&latitude=VALUE&longitude=VALUE&weight_attribute=VALUE&[print_stats=VALUE]\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
