{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\common\\auth.py:130: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  datetime.utcfromtimestamp(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mbf9r14leo8cssfp0c5etnn2rt0a2spq', 1734108651, '2024-12-13 16:50:51')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLUSTER GORDIAS LIS \n",
    "import pyTigerGraph as tg\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "hostName = \"https://2cc2f8bde8df444bb60c6fb83491bb8c.i.tgcloud.io\"\n",
    "graphName = \"VWG\"\n",
    "secret = \"m2p8nba0uab7dtthbn4o1b30r8tqgg9a\"\n",
    "userName = \"user_1\"\n",
    "password = \"A1z2e3r4*\"\n",
    "conn = tg.TigerGraphConnection(host=hostName, graphname=graphName, username=userName, password=password)\n",
    "conn.getToken(secret)# tigergraph_insights_map = \"https://tools.tgcloud.io/insights/app/qepJkoYLXfcWTB4d5ExgVT/page/4NjQNvfLhhTXHgQZBkyxtM/widgetShare/6wci2PjrXvzciTfmrH9pQY?domain=d6e9eef375704c8893b47bdc6132b082.i&orgName=lis-gordias&clusterid=507badf1-6ed3-4a79-9aaa-82489ed609ef&TigerGraphToken=e4120fbd-1fb5-4383-a727-33fb7032eb53\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tab 0 (cargar datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "atributes_nodes = {}\n",
    "atributes_edges = {}\n",
    "\n",
    "# Helper functions\n",
    "def path_taken(results):\n",
    "    # Extrae el conjunto de aristas desde el diccionario de resultados.\n",
    "    edges = results[2]['@@display_edge_set']\n",
    "    \n",
    "    # Inicializa una variable para almacenar el nodo inicial.\n",
    "    node_initial = None\n",
    "    \n",
    "    # Crea un conjunto de identificadores de nodos a los que se dirige cada arista.\n",
    "    edge_to_ids = {edge['to_id'] for edge in edges}\n",
    "    \n",
    "    # Busca el nodo inicial (el nodo que no es el destino de ningún arista).\n",
    "    for edge in edges:\n",
    "        if edge['from_id'] not in edge_to_ids:\n",
    "            node_initial = edge['from_id']\n",
    "            break\n",
    "    \n",
    "    # Si no se encuentra un nodo inicial, lanza una excepción.\n",
    "    if node_initial is None:\n",
    "        raise ValueError(\"initial node cannot be determined.\")\n",
    "    \n",
    "    # Inicializa el nodo actual como el nodo inicial y crea una lista para el camino.\n",
    "    current_node = node_initial\n",
    "    path = [current_node]\n",
    "    \n",
    "    # Recorre los bordes para construir el camino desde el nodo inicial.\n",
    "    while True:\n",
    "        next_node = None\n",
    "        \n",
    "        # Busca el siguiente nodo en el camino basado en el nodo actual.\n",
    "        for edge in edges:\n",
    "            if edge['from_id'] == current_node:\n",
    "                next_node = edge['to_id']\n",
    "                break\n",
    "        \n",
    "        # Si no se encuentra el siguiente nodo, termina el bucle.\n",
    "        if next_node is None:\n",
    "            break\n",
    "        \n",
    "        # Añade el siguiente nodo al camino y actualiza el nodo actual.\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "    \n",
    "    # Crea un diccionario de nodos mapeando cada nodo a sí mismo.\n",
    "    nodes = {node_name: node_name for node_name in path}\n",
    "    \n",
    "    # Crea una lista ordenada de nodos basada en el camino encontrado.\n",
    "    ordered_nodes = [nodes[node_name] for node_name in path]\n",
    "    \n",
    "    # Devuelve la lista de nodos ordenados que representa el camino tomado.\n",
    "    return ordered_nodes\n",
    "\n",
    "\n",
    "def add_or_update_final_nodes_batch(nodes_batch_full, nodes_batch, charge):\n",
    "    # Crea una lista de ciudades existentes a partir de los nodos en 'nodes_batch_full'.\n",
    "    existing_cities = [city for city, _ in nodes_batch_full]\n",
    "    \n",
    "    # Itera sobre cada nodo en el 'nodes_batch' para actualizar o agregar nodos en 'nodes_batch_full'.\n",
    "    for node, _ in nodes_batch:\n",
    "        if node in existing_cities:\n",
    "            # Si el nodo ya existe en 'nodes_batch_full', busca el índice del nodo para actualizar el stock.\n",
    "            for i, (city, stock) in enumerate(nodes_batch_full):\n",
    "                if city == node:\n",
    "                    # Si es el primer nodo, decrementa el stock por 'charge'.\n",
    "                    # De lo contrario, incrementa el stock por 'charge'.\n",
    "                    if i == 0:\n",
    "                        nodes_batch_full[i] = (city, {\"Stock\": nodes_batch_full[i][1][\"Stock\"] - charge})\n",
    "                    else:\n",
    "                        nodes_batch_full[i] = (city, {\"Stock\": nodes_batch_full[i][1][\"Stock\"] + charge})\n",
    "            continue\n",
    "        else:\n",
    "            # Si el nodo no está en 'nodes_batch_full', agrega nuevas entradas para los nodos de 'nodes_batch'.\n",
    "            nodes_batch_full.extend([\n",
    "                (nodes_batch[0][0], {\"Stock\": nodes_batch[0][1][\"Stock\"]}),\n",
    "                (nodes_batch[1][0], {\"Stock\": nodes_batch[1][1][\"Stock\"]})\n",
    "            ])\n",
    "            # Retorna 'nodes_batch_full' después de agregar las nuevas entradas.\n",
    "            return nodes_batch_full\n",
    "    \n",
    "    # Retorna 'nodes_batch_full' después de actualizar o agregar los nodos.\n",
    "    return nodes_batch_full\n",
    "\n",
    "def add_or_update_final_edges_batch(edges_batch_full, edges_batch, charge):\n",
    "    # Crea una lista de bordes existentes a partir de 'edges_batch_full', considerando ambos sentidos.\n",
    "    existing_edges = [(edge[0], edge[1]) for edge in edges_batch_full]\n",
    "    \n",
    "    # Itera sobre cada borde en 'edges_batch' para actualizar o agregar bordes en 'edges_batch_full'.\n",
    "    for edge in edges_batch:\n",
    "        # Verifica si el borde ya existe en 'edges_batch_full' considerando ambos sentidos.\n",
    "        if (edge[0], edge[1]) in existing_edges or (edge[1], edge[0]) in existing_edges:\n",
    "            # Si el borde ya existe, busca el índice del borde para actualizar el movimiento diario.\n",
    "            for i, (origin, destination, movement) in enumerate(edges_batch_full):\n",
    "                # Compara ambos sentidos del borde para encontrar el índice correcto.\n",
    "                if (origin, destination) == (edge[0], edge[1]) or (origin, destination) == (edge[1], edge[0]):\n",
    "                    # Actualiza el movimiento diario del borde sumando 'charge'.\n",
    "                    edges_batch_full[i] = (origin, destination, {\"Daily_movement\": edges_batch_full[i][2][\"Daily_movement\"] + charge})\n",
    "            continue\n",
    "        else:\n",
    "            # Si el borde no existe, agrega una nueva entrada en 'edges_batch_full'.\n",
    "            edges_batch_full.append((edge[0], edge[1], {\"Daily_movement\": edge[2][\"Daily_movement\"]}))\n",
    "    \n",
    "    # Retorna 'edges_batch_full' después de actualizar o agregar los bordes.\n",
    "    return edges_batch_full\n",
    "\n",
    "\n",
    "def actualize_graph_carga(conn=conn):\n",
    "    # global aristas_sobrecargadas\n",
    "    # Obtiene la lista de nodos y aristas desde la conexión proporcionada.\n",
    "    nodes = conn.getVertices(\"Nodes\")\n",
    "    aristas = conn.getEdgesByType(edgeType=\"distribute_to\")\n",
    "\n",
    "    # Inicializa listas para almacenar nodos y aristas con datos escalados.\n",
    "    nodes_batch_scale = []\n",
    "    aristas_batch_scale = []\n",
    "    \n",
    "    # Inicializa una variable para registrar los estados de nodos y aristas.\n",
    "    logs_state_nodes_and_arristas = \"\"\n",
    "\n",
    "    # Contadores para los diferentes niveles de carga de nodos y aristas.\n",
    "    node_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0}\n",
    "    arista_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0}\n",
    "\n",
    "    # Itera sobre cada nodo para determinar su nivel de carga.\n",
    "    for node in nodes:\n",
    "        node_capacidad = node[\"attributes\"][\"Capacity\"]\n",
    "        node_stock = node[\"attributes\"][\"Stock\"]\n",
    "        \n",
    "        # Determina el nivel de carga del nodo basado en su capacidad y stock.\n",
    "        if node_capacidad == 0:\n",
    "            nivel_stock = \"No definido\"\n",
    "        else:\n",
    "            if (node_stock / node_capacidad) > 1:\n",
    "                nivel_stock = \"Sobrecargado\"\n",
    "                node_counts[\"Sobrecargado\"] += 1\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ {node['v_id']} es {nivel_stock} \\n\"\n",
    "            elif 0.7 < (node_stock / node_capacidad) <= 1:\n",
    "                nivel_stock = \"Optimal\"\n",
    "                node_counts[\"Optimal\"] += 1\n",
    "            else:\n",
    "                nivel_stock = \"Subcargado\"\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ {node['v_id']} es {nivel_stock} \\n\"\n",
    "                node_counts[\"Subcargado\"] += 1\n",
    "\n",
    "        # Añade el nodo con su nivel de carga a la lista de nodos a escalar.\n",
    "        nodes_batch_scale.append((node[\"v_id\"], {\"Carga\": nivel_stock}))\n",
    "\n",
    "    # Itera sobre cada arista para determinar su nivel de carga.\n",
    "    for arista in aristas:\n",
    "        capacity_edge = arista[\"attributes\"][\"Capacity\"]\n",
    "        daily_movement = arista[\"attributes\"][\"Daily_movement\"]\n",
    "\n",
    "        # Determina el nivel de carga de la arista basado en su capacidad y movimiento diario.\n",
    "        if capacity_edge == 0:\n",
    "            nivel_arista = \"No definido\"\n",
    "        else:\n",
    "            if (daily_movement / capacity_edge) > 1:\n",
    "                nivel_arista = \"Sobrecargado\"\n",
    "                arista_counts[\"Sobrecargado\"] += 1\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ la arista {arista['from_id']} hasta {arista['to_id']} es {nivel_arista} \\n\"\n",
    "                aristas_sobrecargadas.append(arista)\n",
    "            elif 0.7 < (daily_movement / capacity_edge) <= 1:\n",
    "                nivel_arista = \"Optimal\"\n",
    "                arista_counts[\"Optimal\"] += 1\n",
    "            else:\n",
    "                nivel_arista = \"Subcargado\"\n",
    "                logs_state_nodes_and_arristas += f\"⚠️ la arista {arista['from_id']} hasta {arista['to_id']} es {nivel_arista} \\n\"\n",
    "                arista_counts[\"Subcargado\"] += 1\n",
    "\n",
    "        # Añade la arista con su nivel de carga a la lista de aristas a escalar.\n",
    "        aristas_batch_scale.append((arista[\"from_id\"], arista[\"to_id\"], {\"Carga\": nivel_arista}))\n",
    "\n",
    "    # Actualiza los nodos en la base de datos con los datos escalados.\n",
    "    conn.upsertVertices(\"Nodes\", nodes_batch_scale)\n",
    "    \n",
    "    # Actualiza las aristas en la base de datos con los datos escalados.\n",
    "    conn.upsertEdges(sourceVertexType=\"Nodes\", targetVertexType=\"Nodes\", edgeType=\"distribute_to\", edges=aristas_batch_scale)\n",
    "\n",
    "    # Si no se registraron cambios, indica que todos los nodos y aristas están en un estado óptimo.\n",
    "    if logs_state_nodes_and_arristas == \"\":\n",
    "        logs_state_nodes_and_arristas = \"Todos los nodos y aristas están en un estado óptimo.\"\n",
    "\n",
    "    # Crea un diccionario con los registros de estado y los conteos de nodos y aristas.\n",
    "    results = {\n",
    "        \"logs\": logs_state_nodes_and_arristas,\n",
    "        \"node_counts\": node_counts,\n",
    "        \"arista_counts\": arista_counts,\n",
    "        \"aristas_sobrecargadas\" : aristas_sobrecargadas\n",
    "    }\n",
    "\n",
    "    # Retorna el diccionario con los resultados.\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main transfer function\n",
    "\n",
    "def transfert_nodes_and_edges(node_initial, node_final, charge, weight_attribute=\"Price\", nodetype=\"Nodes\", edgetype=\"distribute_to\"):\n",
    "    global palets_cost, log_list , entregas_and_paths\n",
    "    # Lanza la función A* para encontrar el camino óptimo entre node_initial y node_final\n",
    "    results_astar = conn.runInstalledQuery(\"tg_astar\", params={\n",
    "        \"source_vertex\": node_final, \"source_vertex.type\": nodetype,\n",
    "        \"target_vertex\": node_initial, \"target_vertex.type\": nodetype,\n",
    "        \"e_type_set\": edgetype, \"weight_type\": \"FLOAT\",\n",
    "        \"latitude\": \"latitude\", \"longitude\": \"longitude\",\n",
    "        \"weight_attribute\": weight_attribute,\n",
    "        \"print_stats\": \"True\"\n",
    "    })\n",
    "\n",
    "    key_csv_entrega = f\"{node_initial}_{node_final}_{charge}\"\n",
    "\n",
    "    # Obtiene el orden de los nodos en el camino encontrado\n",
    "    order_taken = path_taken(results_astar)\n",
    "    \n",
    "    # Obtiene el conjunto de aristas involucradas en el camino\n",
    "    edge_set = results_astar[2]['@@display_edge_set']\n",
    "    \n",
    "    # Reorganiza las aristas de acuerdo al orden de los nodos en el camino\n",
    "    reordered_edges_set = []\n",
    "    for location in order_taken:\n",
    "        for edge in edge_set:\n",
    "            if edge['from_id'] == location:\n",
    "                reordered_edges_set.append(edge)\n",
    "\n",
    "    # Obtiene el conjunto de nodos involucrados en el camino\n",
    "    node_set = results_astar[2]['tmp']\n",
    "    \n",
    "    # Reorganiza los nodos de acuerdo al orden en el camino\n",
    "    reordered_nodes_set = []\n",
    "    for location in order_taken:\n",
    "        for item in node_set:\n",
    "            if item['v_id'] == location:\n",
    "                reordered_nodes_set.append(item)\n",
    "    entregas_and_paths[key_csv_entrega] = reordered_nodes_set    # Guardamos todos los paths to check which path can give problem de sobrecarga . \n",
    "    try:\n",
    "        # Recupera los atributos de los nodos inicial y final\n",
    "        node_i_stock = reordered_nodes_set[0][\"attributes\"][\"Stock\"]\n",
    "        node_f_stock = reordered_nodes_set[-1][\"attributes\"][\"Stock\"]\n",
    "        node_i_unload_capacity = reordered_nodes_set[0][\"attributes\"][\"UnloadCapacity\"]\n",
    "        node_f_load_capacity = reordered_nodes_set[-1][\"attributes\"][\"LoadCapacity\"]\n",
    "\n",
    "        # Verifica si hay suficiente inventario en el nodo inicial\n",
    "        with lock:\n",
    "            if charge > node_i_stock:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente inventario ({charge} > {node_i_stock})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Verifica si hay suficiente capacidad de carga en el nodo final\n",
    "            if charge > node_f_load_capacity:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de carga en el almacén de recepción (charge: {charge} > LoadCapacity: {node_f_load_capacity})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Verifica si hay suficiente capacidad de descarga en el nodo inicial\n",
    "            if charge > node_i_unload_capacity:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de descarga desde el almacén de salida (charge: {charge} > UnloadCapacity: {node_i_unload_capacity})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Recupera la capacidad de la arista y el movimiento diario actual\n",
    "            capacity_edge = reordered_edges_set[0][\"attributes\"][\"Capacity\"]\n",
    "            daily_movement = reordered_edges_set[0][\"attributes\"][\"Daily_movement\"]\n",
    "\n",
    "            # Verifica si la arista puede soportar el movimiento adicional de carga\n",
    "            if daily_movement + charge > capacity_edge:\n",
    "                error_message = f\"Enviar {charge} palets desde {node_initial} hasta {node_final}: Capacidad de la arista insuficiente ({daily_movement + charge} > {capacity_edge})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "        # Actualiza los inventarios de los nodos inicial y final\n",
    "        nodes_batch = [\n",
    "            (node_initial, {\"Stock\": node_i_stock - charge}),\n",
    "            (node_final, {\"Stock\": node_f_stock + charge})\n",
    "        ]\n",
    "\n",
    "        # Actualiza el movimiento diario en las aristas recorridas y acumula el costo\n",
    "        edges_batch = []\n",
    "        with lock : \n",
    "            for element in reordered_edges_set:\n",
    "                element['attributes']['Daily_movement'] += charge\n",
    "                edges_batch.append((element[\"from_id\"], element[\"to_id\"], {\"Daily_movement\": element[\"attributes\"][\"Daily_movement\"]}))\n",
    "                palets_cost += element[\"attributes\"][\"Price\"]\n",
    "\n",
    "        # Añade los nodos inicial y final a los conjuntos de nodos únicos\n",
    "        unique_origin_nodes.add(node_initial)\n",
    "        unique_destination_nodes.add(node_final)\n",
    "\n",
    "        return nodes_batch , edges_batch \n",
    "\n",
    "    # Manejo de excepciones en caso de que ocurra algún error durante el proceso\n",
    "    except Exception as e:\n",
    "        log_list += f\"Error durante la transferencia de {node_initial} a {node_final}: {str(e)}\\n\"\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error en la transferancia: {e}\\n\"\n",
    "        log_list += error_message\n",
    "        return error_message\n",
    "\n",
    "\n",
    "# Thread safety\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def make_daily_movements(input_file, column_origin, column_destination, column_transfert):\n",
    "    # Variables globales para rastrear estadísticas y almacenar datos.\n",
    "    global aristas_sobrecargadas,entregas_and_paths,operation_count, palets_number, palets_cost, unique_origin_nodes, unique_destination_nodes, log_list, nodes_batch_full, edges_batch_full, df\n",
    "    operation_count = 0  # Conteo de operaciones realizadas.\n",
    "    palets_number = 0  # Número total de palets transferidos.\n",
    "    palets_cost = 0  # Costo total asociado a los palets transferidos.\n",
    "    unique_origin_nodes = set()  # Conjunto de nodos de origen únicos.\n",
    "    unique_destination_nodes = set()  # Conjunto de nodos de destino únicos.\n",
    "    log_list = \" \"  # Registro de errores o mensajes importantes.\n",
    "    nodes_batch_full = []  # Lista para almacenar nodos actualizados.\n",
    "    edges_batch_full = []  # Lista para almacenar aristas actualizadas.\n",
    "    aristas_sobrecargadas = []\n",
    "    entregas_and_paths = {}\n",
    "\n",
    "\n",
    "    # Lee el archivo CSV y carga los datos en un DataFrame.\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Función para manejar la transferencia de volumen entre nodos.\n",
    "    def perform_transfer(row):\n",
    "        global log_list,operation_count, palets_number\n",
    "        try:\n",
    "            # Obtiene el origen, destino y volumen desde la fila del DataFrame.\n",
    "            origin = str(row[column_origin])\n",
    "            destination = str(row[column_destination])\n",
    "            volume = float(row[column_transfert])\n",
    "            \n",
    "            # Llama a la función para obtener nodos y aristas actualizados.\n",
    "            nodes_batch, edges_batch = transfert_nodes_and_edges(origin, destination, volume)\n",
    "            \n",
    "            # Usa un lock para garantizar que las actualizaciones a las listas sean seguras en un entorno multihilo.\n",
    "            with lock:\n",
    "                # Actualiza o agrega nodos y aristas a las listas correspondientes.\n",
    "                add_or_update_final_nodes_batch(nodes_batch_full, nodes_batch, volume)\n",
    "                add_or_update_final_edges_batch(edges_batch_full, edges_batch, volume)\n",
    "            \n",
    "            # Actualiza las métricas globales.\n",
    "            operation_count += 1  # Incrementa el conteo de operaciones.\n",
    "            palets_number += volume  # Incrementa el número total de palets.\n",
    "            unique_origin_nodes.add(origin)  # Agrega el nodo de origen al conjunto de nodos únicos.\n",
    "            unique_destination_nodes.add(destination)  # Agrega el nodo de destino al conjunto de nodos únicos.\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Registra un mensaje de error si ocurre una excepción.\n",
    "            error_message = f\"{origin} hasta {destination}, no camino posible.\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "    # Usa un ThreadPoolExecutor para procesar las transferencias en paralelo.\n",
    "    with ThreadPoolExecutor(max_workers=1000) as executor:\n",
    "        # Crea y envía tareas para cada fila del DataFrame.\n",
    "        futures = [executor.submit(perform_transfer, row) for index, row in df.iterrows()]\n",
    "        # Espera a que todas las tareas se completen.\n",
    "        for future in as_completed(futures):\n",
    "            pass  # Se pueden manejar los resultados de las tareas aquí si es necesario.\n",
    "\n",
    "    # Actualiza los nodos y aristas en la base de datos.\n",
    "    conn.upsertVertices(\"Nodes\", nodes_batch_full)\n",
    "    nodetype = \"Nodes\"\n",
    "    edgetype = \"distribute_to\"\n",
    "    conn.upsertEdges(sourceVertexType=nodetype, targetVertexType=nodetype, edgeType=edgetype, edges=edges_batch_full)\n",
    "    \n",
    "    # Llama a la función para actualizar el gráfico con el estado actual y obtiene el estado.\n",
    "    state_log = actualize_graph_carga(conn)\n",
    "\n",
    "    # Si no hubo errores, actualiza el mensaje de log para indicar éxito.\n",
    "    if log_list == \" \":\n",
    "        log_list = \"All operations were successful.\"\n",
    "\n",
    "    print(\"Número de operaciones realizadas:\", operation_count)\n",
    "    print(\"Número total de palets transferidos:\", palets_number)\n",
    "    print(\"Costo total asociado a los palets transferidos:\", palets_cost)\n",
    "    print(\"Conjunto de nodos de origen únicos:\", unique_origin_nodes)\n",
    "    print(\"Conjunto de nodos de destino únicos:\", unique_destination_nodes)\n",
    "    print(\"Registro de errores o mensajes importantes:\", log_list)\n",
    "    print(\"Lista de nodos actualizados:\", nodes_batch_full)\n",
    "    print(\"Lista de aristas actualizadas:\", edges_batch_full)\n",
    "    print(f\"log_list: {log_list}\")\n",
    "    print(f\"state log : {state_log}\")\n",
    "\n",
    "\n",
    "    # Devuelve un conjunto de resultados incluyendo logs, un botón para recargar el mapa y KPIs actualizados.\n",
    "    return (\n",
    "        log_list,\n",
    "        # reset_html(),  # Asume que esta función devuelve HTML para reiniciar la interfaz.\n",
    "        gr.Button(\"Recargar mapa\", elem_classes=\"otherbutton\", visible=True),\n",
    "        gr.Number(label=\"KPI : Numero de operaciones\", value=operation_count, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Numero de palets\", value=palets_number, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Numero de origen\", value=len(unique_origin_nodes), visible=True, elem_classes=\"dropdown\"),  \n",
    "        gr.Number(label=\"KPI : Destinaciones diferentes\", value=len(unique_destination_nodes), visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Number(label=\"KPI : Coste total\", value=palets_cost, visible=True, elem_classes=\"dropdown\"), \n",
    "        gr.Number(label=\"KPI : Coste por palet\", value=round(palets_cost / 1, 1), visible=True, elem_classes=\"dropdown\") \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de operaciones realizadas: 82\n",
      "Número total de palets transferidos: 546.0\n",
      "Costo total asociado a los palets transferidos: 1715\n",
      "Conjunto de nodos de origen únicos: {'Kiev', 'Stockholm', 'Bilbao', 'Krakow', 'Madrid', 'Skopje', 'Zagreb', 'Berlin', 'Nantes', 'Milan', 'London', 'Paris', 'Vilnius'}\n",
      "Conjunto de nodos de destino únicos: {'Bologna', 'Malmo', 'Glasgow', 'Brno', 'Bruxelles', 'Warsaw', 'Bremen', 'Moscow', 'Uppsala', 'Graz', 'Dublin', 'Constanta', 'Barcelona', 'Patras', 'Tallinn', 'Odessa', 'Rotterdam', 'Kiev', 'Split', 'Valencia', 'Bucharest', 'Cologne', 'Utrecht', 'La Coruna', 'Genoa', 'Stavanger', 'Vigo', 'Hamburg', 'Rome', 'Strasbourg', 'Chisinau', 'Helsinki', 'Birmingham', 'Turin', 'Santander', 'Louvain', 'Athens', 'Zaragoza', 'Naples', 'Skopje', 'Ljubljana', 'Belgrade', 'Bergen', 'Sofia', 'Liverpool', 'Cork', 'Frankfurt', 'Belfast', 'Kharkiv', 'Pristina', 'Paris', 'Aalborg', 'Sarajevo', 'Manchester', 'Munich', 'Minsk', 'Bilbao', 'Seville', 'Oslo', 'Prague', 'Valladolid', 'Riga', 'Copenhague', 'Berlin', 'Mostar', 'Amsterdam', 'Vienna', 'Thessaloniki', 'Venice'}\n",
      "Registro de errores o mensajes importantes:  Skopje hasta Belgrade: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 8.0 > UnloadCapacity: 6)\n",
      "Bilbao hasta Paris: No hay suficiente capacidad de carga en el almacén de recepción (charge: 45.0 > LoadCapacity: 19)\n",
      "Bilbao hasta Paris: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 45.0 > UnloadCapacity: 16)\n",
      "Skopje hasta Sofia: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "Skopje hasta Athens: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 8.0 > UnloadCapacity: 6)\n",
      "Stockholm hasta Stavanger: No hay suficiente capacidad de carga en el almacén de recepción (charge: 10.0 > LoadCapacity: 8)\n",
      "Skopje hasta Constanta: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "Skopje hasta Belgrade: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "\n",
      "Lista de nodos actualizados: [('Paris', {'Stock': 200.0}), ('Louvain', {'Stock': 47.0}), ('Madrid', {'Stock': 167.6}), ('Zaragoza', {'Stock': 62.0}), ('Vilnius', {'Stock': 103.0}), ('Kharkiv', {'Stock': 64.0}), ('Paris', {'Stock': 266.0}), ('Strasbourg', {'Stock': 76.0}), ('Vilnius', {'Stock': 91.0}), ('Minsk', {'Stock': 91.0}), ('Berlin', {'Stock': 116.0}), ('Munich', {'Stock': 82.0}), ('Zagreb', {'Stock': 103.0}), ('Sarajevo', {'Stock': 68.0}), ('Madrid', {'Stock': 153.6}), ('Santander', {'Stock': 30.8}), ('Kiev', {'Stock': 114.0}), ('Kharkiv', {'Stock': 62.0}), ('Zagreb', {'Stock': 99.0}), ('Split', {'Stock': 35.0}), ('Skopje', {'Stock': 73.0}), ('Belgrade', {'Stock': 93.0}), ('Milan', {'Stock': 101.0}), ('Turin', {'Stock': 71.0}), ('London', {'Stock': 107.0}), ('Birmingham', {'Stock': 83.0}), ('Paris', {'Stock': 260.0}), ('Utrecht', {'Stock': 66.0}), ('Bilbao', {'Stock': 59.3}), ('Paris', {'Stock': 311.0}), ('Skopje', {'Stock': 69.0}), ('Patras', {'Stock': 59.0}), ('Krakow', {'Stock': 97.0}), ('Prague', {'Stock': 45.0}), ('London', {'Stock': 108.0}), ('Manchester', {'Stock': 84.0}), ('Paris', {'Stock': 260.0}), ('Rotterdam', {'Stock': 76.0}), ('Skopje', {'Stock': 57.0}), ('Sofia', {'Stock': 84.0}), ('Milan', {'Stock': 101.0}), ('Bologna', {'Stock': 65.0}), ('Milan', {'Stock': 89.0}), ('Naples', {'Stock': 72.0}), ('Skopje', {'Stock': 62.0}), ('Pristina', {'Stock': 60.0}), ('Krakow', {'Stock': 84.0}), ('Brno', {'Stock': 61.0}), ('Krakow', {'Stock': 74.0}), ('Warsaw', {'Stock': 105.0}), ('Madrid', {'Stock': 149.6}), ('Vigo', {'Stock': 51.0}), ('Berlin', {'Stock': 103.0}), ('Bremen', {'Stock': 90.0}), ('Paris', {'Stock': 255.0}), ('Frankfurt', {'Stock': 111.0}), ('Kiev', {'Stock': 106.0}), ('Chisinau', {'Stock': 45.0}), ('Vilnius', {'Stock': 89.0}), ('Tallinn', {'Stock': 73.0}), ('Skopje', {'Stock': 48.0}), ('Athens', {'Stock': 81.0}), ('Madrid', {'Stock': 132.6}), ('Valladolid', {'Stock': 75.2}), ('London', {'Stock': 100.0}), ('Dublin', {'Stock': 76.0}), ('Vilnius', {'Stock': 76.0}), ('Odessa', {'Stock': 109.0}), ('Stockholm', {'Stock': 127.0}), ('Aalborg', {'Stock': 49.0}), ('Milan', {'Stock': 82.0}), ('Genoa', {'Stock': 63.0}), ('Madrid', {'Stock': 123.6}), ('Seville', {'Stock': 150.7}), ('Madrid', {'Stock': 111.6}), ('Valencia', {'Stock': 14.0}), ('Vilnius', {'Stock': 77.0}), ('Bucharest', {'Stock': 106.0}), ('Stockholm', {'Stock': 116.0}), ('Stavanger', {'Stock': 30.0}), ('Stockholm', {'Stock': 114.0}), ('Helsinki', {'Stock': 86.0}), ('London', {'Stock': 100.0}), ('Cork', {'Stock': 49.0}), ('Berlin', {'Stock': 95.0}), ('Hamburg', {'Stock': 93.0}), ('Skopje', {'Stock': 36.0}), ('Constanta', {'Stock': 63.0}), ('Paris', {'Stock': 235.0}), ('Bruxelles', {'Stock': 85.0}), ('Vilnius', {'Stock': 63.0}), ('Riga', {'Stock': 64.0}), ('Stockholm', {'Stock': 100.0}), ('Malmo', {'Stock': 60.0}), ('Stockholm', {'Stock': 102.0}), ('Uppsala', {'Stock': 44.0}), ('Vilnius', {'Stock': 49.0}), ('Moscow', {'Stock': 101.0}), ('Nantes', {'Stock': 82.0}), ('Paris', {'Stock': 248.0}), ('Zagreb', {'Stock': 73.0}), ('Ljubljana', {'Stock': 43.0}), ('Milan', {'Stock': 83.0}), ('Venice', {'Stock': 73.0}), ('London', {'Stock': 95.0}), ('Glasgow', {'Stock': 69.0}), ('Madrid', {'Stock': 101.6}), ('Barcelona', {'Stock': 88.1}), ('Zagreb', {'Stock': 58.0}), ('Vienna', {'Stock': 93.0}), ('Berlin', {'Stock': 87.0}), ('Cologne', {'Stock': 68.2}), ('London', {'Stock': 87.0}), ('Belfast', {'Stock': 72.0}), ('Stockholm', {'Stock': 86.0}), ('Oslo', {'Stock': 50.0}), ('Milan', {'Stock': 78.0}), ('Rome', {'Stock': 82.0}), ('Paris', {'Stock': 235.0}), ('Amsterdam', {'Stock': 77.4}), ('Skopje', {'Stock': 22.0}), ('Thessaloniki', {'Stock': 63.0}), ('Stockholm', {'Stock': 88.0}), ('Copenhague', {'Stock': 81.0}), ('Stockholm', {'Stock': 88.0}), ('Bergen', {'Stock': 65.0}), ('Zagreb', {'Stock': 44.0}), ('Graz', {'Stock': 65.0}), ('London', {'Stock': 77.0}), ('Liverpool', {'Stock': 84.0}), ('Zagreb', {'Stock': 42.0}), ('Mostar', {'Stock': 43.0}), ('Madrid', {'Stock': 99.6}), ('La Coruna', {'Stock': 57.6})]\n",
      "Lista de aristas actualizadas: [('Paris', 'Amsterdam', {'Daily_movement': 26.0}), ('Amsterdam', 'Bruxelles', {'Daily_movement': 12.0}), ('Bruxelles', 'Louvain', {'Daily_movement': 2.0}), ('Madrid', 'Zaragoza', {'Daily_movement': 2.0}), ('Vilnius', 'Moscow', {'Daily_movement': 36.0}), ('Moscow', 'Kharkiv', {'Daily_movement': 27.0}), ('Paris', 'Strasbourg', {'Daily_movement': 10.0}), ('Vilnius', 'Minsk', {'Daily_movement': 8.0}), ('Berlin', 'Munich', {'Daily_movement': 14.0}), ('Zagreb', 'Split', {'Daily_movement': 51.0}), ('Split', 'Mostar', {'Daily_movement': 48.0}), ('Mostar', 'Sarajevo', {'Daily_movement': 2.0}), ('Madrid', 'Bilbao', {'Daily_movement': 17.0}), ('Bilbao', 'Santander', {'Daily_movement': 8.0}), ('Kiev', 'Kharkiv', {'Daily_movement': 25.0}), ('Skopje', 'Pristina', {'Daily_movement': 42.0}), ('Pristina', 'Podgorica', {'Daily_movement': 48.0}), ('Podgorica', 'Mostar', {'Daily_movement': 41.0}), ('Zagreb', 'Belgrade', {'Daily_movement': 23.0}), ('Milan', 'Turin', {'Daily_movement': 4.0}), ('London', 'Birmingham', {'Daily_movement': 32.0}), ('Amsterdam', 'Utrecht', {'Daily_movement': 6.0}), ('Bilbao', 'Toulouse', {'Daily_movement': 49.0}), ('Toulouse', 'Clermont-Ferrand', {'Daily_movement': 49.0}), ('Clermont-Ferrand', 'Paris', {'Daily_movement': 49.0}), ('Skopje', 'Thessaloniki', {'Daily_movement': 20.0}), ('Thessaloniki', 'Patras', {'Daily_movement': 14.0}), ('Krakow', 'Olomouc', {'Daily_movement': 17.0}), ('Olomouc', 'Prague', {'Daily_movement': 17.0}), ('London', 'Manchester', {'Daily_movement': 4.0}), ('Amsterdam', 'Rotterdam', {'Daily_movement': 3.0}), ('Skopje', 'Sofia', {'Daily_movement': 41.0}), ('Milan', 'Bologna', {'Daily_movement': 2.0}), ('Milan', 'Rome', {'Daily_movement': 11.0}), ('Rome', 'Naples', {'Daily_movement': 7.0}), ('Prague', 'Brno', {'Daily_movement': 10.0}), ('Krakow', 'Vilnius', {'Daily_movement': 17.0}), ('Vilnius', 'Warsaw', {'Daily_movement': 17.0}), ('Madrid', 'Valladolid', {'Daily_movement': 19.0}), ('Valladolid', 'La Coruna', {'Daily_movement': 12.0}), ('La Coruna', 'Vigo', {'Daily_movement': 6.0}), ('Berlin', 'Hamburg', {'Daily_movement': 19.0}), ('Hamburg', 'Bremen', {'Daily_movement': 10.0}), ('Strasbourg', 'Frankfurt', {'Daily_movement': 4.0}), ('Kiev', 'Odessa', {'Daily_movement': 31.0}), ('Odessa', 'Chisinau', {'Daily_movement': 5.0}), ('Vilnius', 'Riga', {'Daily_movement': 9.0}), ('Riga', 'Tallinn', {'Daily_movement': 5.0}), ('Patras', 'Athens', {'Daily_movement': 8.0}), ('Birmingham', 'Liverpool', {'Daily_movement': 23.0}), ('Liverpool', 'Dublin', {'Daily_movement': 15.0}), ('Stockholm', 'Malmo', {'Daily_movement': 23.0}), ('Malmo', 'Copenhague', {'Daily_movement': 13.0}), ('Copenhague', 'Aalborg', {'Daily_movement': 9.0}), ('Milan', 'Genoa', {'Daily_movement': 7.0}), ('Madrid', 'Seville', {'Daily_movement': 8.0}), ('Madrid', 'Valencia', {'Daily_movement': 10.0}), ('Odessa', 'Podgorica', {'Daily_movement': 7.0}), ('Sofia', 'Bucharest', {'Daily_movement': 32.0}), ('Stockholm', 'Oslo', {'Daily_movement': 22.0}), ('Oslo', 'Bergen', {'Daily_movement': 12.0}), ('Bergen', 'Stavanger', {'Daily_movement': 10.0}), ('Stockholm', 'Helsinki', {'Daily_movement': 6.0}), ('Dublin', 'Cork', {'Daily_movement': 3.0}), ('Bucharest', 'Constanta', {'Daily_movement': 16.0}), ('Stockholm', 'Uppsala', {'Daily_movement': 4.0}), ('Munich', 'Frankfurt', {'Daily_movement': 7.0}), ('Nantes', 'Paris', {'Daily_movement': 3.0}), ('Zagreb', 'Ljubljana', {'Daily_movement': 27.0}), ('Milan', 'Venice', {'Daily_movement': 3.0}), ('London', 'Glasgow', {'Daily_movement': 4.0}), ('Madrid', 'Barcelona', {'Daily_movement': 10.0}), ('Ljubljana', 'Graz', {'Daily_movement': 24.0}), ('Graz', 'Vienna', {'Daily_movement': 16.0}), ('Berlin', 'Cologne', {'Daily_movement': 5.0}), ('Dublin', 'Belfast', {'Daily_movement': 6.0}), ('Krakow', 'Budapest', {'Daily_movement': 13.0}), ('Budapest', 'Vienna', {'Daily_movement': 13.0}), ('Warsaw', 'Berlin', {'Daily_movement': 7.0})]\n",
      "log_list:  Skopje hasta Belgrade: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 8.0 > UnloadCapacity: 6)\n",
      "Bilbao hasta Paris: No hay suficiente capacidad de carga en el almacén de recepción (charge: 45.0 > LoadCapacity: 19)\n",
      "Bilbao hasta Paris: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 45.0 > UnloadCapacity: 16)\n",
      "Skopje hasta Sofia: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "Skopje hasta Athens: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 8.0 > UnloadCapacity: 6)\n",
      "Stockholm hasta Stavanger: No hay suficiente capacidad de carga en el almacén de recepción (charge: 10.0 > LoadCapacity: 8)\n",
      "Skopje hasta Constanta: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "Skopje hasta Belgrade: No hay suficiente capacidad de descarga desde el almacén de salida (charge: 9.0 > UnloadCapacity: 6)\n",
      "\n",
      "state log : {'logs': '⚠️ Skopje es Subcargado \\n⚠️ Vilnius es Subcargado \\n⚠️ Kiev es Sobrecargado \\n⚠️ Santander es Sobrecargado \\n⚠️ Bilbao es Subcargado \\n⚠️ la arista Paris hasta Lyon es Subcargado \\n⚠️ la arista Paris hasta Calais es Subcargado \\n⚠️ la arista Krakow hasta Kosice es Subcargado \\n⚠️ la arista Paris hasta Luxembourg es Subcargado \\n⚠️ la arista Valencia hasta Bilbao es Subcargado \\n⚠️ la arista Berne hasta Ljubljana es Subcargado \\n⚠️ la arista Luxembourg hasta Paris es Subcargado \\n⚠️ la arista Valencia hasta Seville es Subcargado \\n⚠️ la arista Ljubljana hasta Berne es Subcargado \\n⚠️ la arista Zaragoza hasta Madrid es Subcargado \\n⚠️ la arista Osijek hasta Zagreb es Subcargado \\n⚠️ la arista Bruxelles hasta Antwerp es Subcargado \\n⚠️ la arista Plovdiv hasta Sofia es Subcargado \\n⚠️ la arista Bratislava hasta Vienna es Subcargado \\n⚠️ la arista Milan hasta Monaco es Subcargado \\n⚠️ la arista Valencia hasta Barcelona es Subcargado \\n⚠️ la arista Budapest hasta Bratislava es Subcargado \\n⚠️ la arista Paris hasta Brussels es Subcargado \\n⚠️ la arista Madrid hasta Zaragoza es Subcargado \\n⚠️ la arista Marseille hasta Monaco es Subcargado \\n⚠️ la arista Sarajevo hasta Mostar es Subcargado \\n⚠️ la arista Sofia hasta Plovdiv es Subcargado \\n⚠️ la arista Barcelona hasta Valencia es Subcargado \\n⚠️ la arista Manchester hasta Birmingham es Subcargado \\n⚠️ la arista Monaco hasta Milan es Subcargado \\n⚠️ la arista Louvain hasta Bruxelles es Subcargado \\n⚠️ la arista Prague hasta Munich es Subcargado \\n⚠️ la arista Antwerp hasta Bruxelles es Subcargado \\n⚠️ la arista Mostar hasta Sarajevo es Subcargado \\n⚠️ la arista Toulouse hasta Marseille es Subcargado \\n⚠️ la arista Helsinki hasta Tallinn es Subcargado \\n⚠️ la arista London hasta Plymouth es Subcargado \\n⚠️ la arista Plymouth hasta London es Subcargado \\n⚠️ la arista Birmingham hasta Manchester es Subcargado \\n⚠️ la arista Vienna hasta Bratislava es Subcargado \\n⚠️ la arista Plymouth hasta Santander es Subcargado \\n⚠️ la arista Toulouse hasta Bilbao es Sobrecargado \\n⚠️ la arista Clermont-Ferrand hasta Toulouse es Sobrecargado \\n⚠️ la arista Lyon hasta Paris es Subcargado \\n⚠️ la arista Hamburg hasta Copenhague es Subcargado \\n⚠️ la arista Clermont-Ferrand hasta Paris es Sobrecargado \\n⚠️ la arista Lyon hasta Marseille es Subcargado \\n⚠️ la arista Paris hasta Clermont-Ferrand es Sobrecargado \\n⚠️ la arista Bilbao hasta Valencia es Subcargado \\n⚠️ la arista Munich hasta Prague es Subcargado \\n⚠️ la arista Bologna hasta Milan es Subcargado \\n⚠️ la arista Marseille hasta Barcelona es Subcargado \\n⚠️ la arista Milan hasta Bologna es Subcargado \\n⚠️ la arista Monaco hasta Marseille es Subcargado \\n⚠️ la arista Calais hasta Paris es Subcargado \\n⚠️ la arista Toulouse hasta Clermont-Ferrand es Sobrecargado \\n⚠️ la arista Barcelona hasta Marseille es Subcargado \\n⚠️ la arista Marseille hasta Lyon es Subcargado \\n⚠️ la arista Zagreb hasta Osijek es Subcargado \\n⚠️ la arista Marseille hasta Toulouse es Subcargado \\n⚠️ la arista Copenhague hasta Hamburg es Subcargado \\n⚠️ la arista Santander hasta Plymouth es Subcargado \\n⚠️ la arista Kosice hasta Krakow es Subcargado \\n⚠️ la arista Bratislava hasta Budapest es Subcargado \\n⚠️ la arista Bruxelles hasta Louvain es Subcargado \\n⚠️ la arista Seville hasta Valencia es Subcargado \\n⚠️ la arista Tallinn hasta Helsinki es Subcargado \\n⚠️ la arista London hasta Paris es Subcargado \\n⚠️ la arista Bilbao hasta Toulouse es Sobrecargado \\n⚠️ la arista Brussels hasta Paris es Subcargado \\n⚠️ la arista Paris hasta London es Subcargado \\n', 'node_counts': {'Sobrecargado': 2, 'Subcargado': 3, 'Optimal': 94}, 'arista_counts': {'Sobrecargado': 6, 'Subcargado': 60, 'Optimal': 144}, 'aristas_sobrecargadas': [{'e_type': 'distribute_to', 'from_id': 'Toulouse', 'from_type': 'Nodes', 'to_id': 'Bilbao', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 5, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}, {'e_type': 'distribute_to', 'from_id': 'Clermont-Ferrand', 'from_type': 'Nodes', 'to_id': 'Toulouse', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 7, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}, {'e_type': 'distribute_to', 'from_id': 'Clermont-Ferrand', 'from_type': 'Nodes', 'to_id': 'Paris', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 3, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}, {'e_type': 'distribute_to', 'from_id': 'Paris', 'from_type': 'Nodes', 'to_id': 'Clermont-Ferrand', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 3, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}, {'e_type': 'distribute_to', 'from_id': 'Toulouse', 'from_type': 'Nodes', 'to_id': 'Clermont-Ferrand', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 7, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}, {'e_type': 'distribute_to', 'from_id': 'Bilbao', 'from_type': 'Nodes', 'to_id': 'Toulouse', 'to_type': 'Nodes', 'directed': False, 'attributes': {'Price': 5, 'Carga': 'Optimal', 'Capacity': 45, 'Daily_movement': 49}}]}\n"
     ]
    }
   ],
   "source": [
    "log_messages = make_daily_movements(r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\CSV_entrega\\entrega_demo_palets_random.csv\", \"CODE_Origin\", \"CODE_Destination\", \"Palets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifiar las entregas que sobrecarguen, y desahacer los cambios hechos por los pedidos que sobrecarguen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Function to extract delivery paths ---\n",
    "\n",
    "def extract_paths_from_deliveries(entregas_and_paths):\n",
    "    \"\"\"\n",
    "    Extracts delivery paths by iterating over each delivery entry, \n",
    "    and organizes the nodes in order of the delivery route.\n",
    "    \"\"\"\n",
    "    paths = {}\n",
    "    for key, delivery_data in entregas_and_paths.items():\n",
    "        path = [node['v_id'] for node in delivery_data]\n",
    "        paths[key] = tuple(path)  # Store the path as a tuple\n",
    "    return paths\n",
    "\n",
    "\n",
    "# --- Function to retrieve overloaded edges ---\n",
    "\n",
    "def get_overloaded_edges(aristas_sobrecargadas):\n",
    "    \"\"\"\n",
    "    Extracts overloaded edges in the format (from_node, to_node, price).\n",
    "    \"\"\"\n",
    "    return {(arista['from_id'], arista['to_id'], arista['attributes']['Price']) for arista in aristas_sobrecargadas}\n",
    "\n",
    "\n",
    "# --- Function to split a path into individual edges ---\n",
    "\n",
    "def split_path_into_edges(path):\n",
    "    \"\"\"\n",
    "    Splits a path into edges, each represented as a tuple of consecutive nodes.\n",
    "    \"\"\"\n",
    "    return [(path[i], path[i + 1]) for i in range(len(path) - 1)]\n",
    "\n",
    "\n",
    "# --- Function to identify overloaded paths ---\n",
    "\n",
    "def check_overloaded_paths(paths, overloaded_edges):\n",
    "    \"\"\"\n",
    "    Checks each path to see if any segment overlaps with an overloaded edge.\n",
    "    Returns a list of overloaded path segments.\n",
    "    \"\"\"\n",
    "    overloaded_segments = []\n",
    "    overloaded_edges_set = {(from_node, to_node) for from_node, to_node, price in overloaded_edges}\n",
    "\n",
    "    for path_key, path in paths.items():\n",
    "        path_edges = split_path_into_edges(path)\n",
    "\n",
    "        for edge in path_edges:\n",
    "            if edge in overloaded_edges_set:\n",
    "                overloaded_segments.append((path_key, edge))\n",
    "\n",
    "    return overloaded_segments\n",
    "\n",
    "\n",
    "# --- Function to filter deliveries causing overloads from CSV data ---\n",
    "\n",
    "def new_dataframe_entrega(csv, overloaded_segments):    \n",
    "    \"\"\"\n",
    "    Reads a CSV file and filters rows based on overloaded delivery segments.\n",
    "    Returns a DataFrame with the filtered rows.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "    filtered_rows = []\n",
    "\n",
    "    for element in overloaded_segments:\n",
    "        name = element[0]  \n",
    "        parts = name.split('_')\n",
    "        origin = parts[0]\n",
    "        destination = '_'.join(parts[1:-1]) \n",
    "        palets = float(parts[-1])\n",
    "\n",
    "        filtered_df = df[\n",
    "            (df['CODE_Origin'] == origin) & \n",
    "            (df['CODE_Destination'] == destination) & \n",
    "            (df['Palets'] == palets)\n",
    "        ]\n",
    "        \n",
    "        filtered_rows.append(filtered_df)\n",
    "\n",
    "    final_filtered_df = pd.concat(filtered_rows, ignore_index=True).drop_duplicates()\n",
    "    return final_filtered_df\n",
    "\n",
    "\n",
    "# --- Function to delete an edge from the graph database ---\n",
    "\n",
    "def erase_edge(conn, source_vertex_id, target_vertex_id, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    \"\"\"\n",
    "    Deletes an edge in the graph database between source and target nodes.\n",
    "    Returns a message indicating success or failure.\n",
    "    \"\"\"\n",
    "    result = conn.delEdges(\n",
    "        sourceVertexType=vertex_type,\n",
    "        targetVertexType=vertex_type,\n",
    "        edgeType=edgeType,\n",
    "        sourceVertexId=source_vertex_id,\n",
    "        targetVertexId=target_vertex_id\n",
    "    )\n",
    "    \n",
    "    if result == {edgeType: 0}:\n",
    "        message = f\"Edge from {source_vertex_id} to {target_vertex_id} does not exist or could not be deleted\\n\"\n",
    "    else:\n",
    "        message = f\"Edge from {source_vertex_id} to {target_vertex_id} deleted\\n\"\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "# --- Function to update the daily movement on each edge ---\n",
    "\n",
    "def update_daily_movement(deliveries):\n",
    "    \"\"\"\n",
    "    Updates the 'Daily_movement' attribute for each unique edge by accumulating\n",
    "    all necessary adjustments to avoid conflicts when multiple deliveries involve\n",
    "    the same edges.\n",
    "    \"\"\"\n",
    "    # Dictionary to accumulate adjustments for each unique edge\n",
    "    edges_updates = defaultdict(lambda: {\"total_palets\": 0, \"attributes\": {}})\n",
    "\n",
    "    # Loop through each delivery to calculate the adjustments\n",
    "    for delivery in deliveries:\n",
    "        palets = delivery['palets']\n",
    "        \n",
    "        # For each edge in the current delivery\n",
    "        for edge in delivery['edges']:\n",
    "            # Unique key for each edge based on from_id and to_id\n",
    "            edge_key = (edge['from_id'], edge['to_id'])\n",
    "            \n",
    "            # Accumulate palets for this edge\n",
    "            edges_updates[edge_key][\"total_palets\"] += palets\n",
    "            \n",
    "            # Store edge attributes only once (shared for each unique edge)\n",
    "            if not edges_updates[edge_key][\"attributes\"]:\n",
    "                edges_updates[edge_key][\"attributes\"] = edge['attributes'].copy()\n",
    "\n",
    "    # Prepare the final list of edges with updated attributes\n",
    "    updated_edges = []\n",
    "    for (from_id, to_id), data in edges_updates.items():\n",
    "        # Get the original edge attributes and make a copy\n",
    "        updated_attributes = data[\"attributes\"].copy()\n",
    "        \n",
    "        # Adjust Daily_movement by subtracting the total accumulated palets\n",
    "        updated_attributes['Daily_movement'] -= data[\"total_palets\"]\n",
    "        \n",
    "        # Append the updated edge tuple (from_id, to_id, updated_attributes) to the list\n",
    "        updated_edges.append((from_id, to_id, updated_attributes))\n",
    "\n",
    "    return updated_edges\n",
    "\n",
    "# --- Main function to orchestrate the entire process ---\n",
    "\n",
    "def main(entregas_and_paths, aristas_sobrecargadas, csv_path, conn):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire process:\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract delivery paths\n",
    "    paths = extract_paths_from_deliveries(entregas_and_paths)\n",
    "\n",
    "    # Step 2: Get overloaded edges\n",
    "    overloaded_edges = get_overloaded_edges(aristas_sobrecargadas)\n",
    "\n",
    "    # Step 3: Identify overloaded paths\n",
    "    overloaded_segments = check_overloaded_paths(paths, overloaded_edges)\n",
    "\n",
    "    # Step 4: Filter deliveries from CSV based on overloaded paths\n",
    "    final_filtered_df = new_dataframe_entrega(csv_path, overloaded_segments)\n",
    "\n",
    "    # Step 5: Run A* algorithm for alternative paths and build undo list\n",
    "    liste_to_undo = []\n",
    "    for index, row in final_filtered_df.iterrows():\n",
    "        node_final = row['CODE_Origin']\n",
    "        node_initial = row['CODE_Destination']\n",
    "        palets = row['Palets']\n",
    "\n",
    "        # Run the A* algorithm\n",
    "        results_astar = conn.runInstalledQuery(\"tg_astar\", params={\n",
    "            \"source_vertex\": node_final,\n",
    "            \"source_vertex.type\": \"Nodes\",\n",
    "            \"target_vertex\": node_initial,\n",
    "            \"target_vertex.type\": \"Nodes\",\n",
    "            \"e_type_set\": \"distribute_to\",\n",
    "            \"weight_type\": \"FLOAT\",\n",
    "            \"latitude\": \"latitude\",\n",
    "            \"longitude\": \"longitude\",\n",
    "            \"weight_attribute\": \"Price\",\n",
    "            \"print_stats\": \"True\"\n",
    "        })\n",
    "\n",
    "        trajet_info = {\n",
    "            'source': node_final,\n",
    "            'destination': node_initial,\n",
    "            'palets': palets,\n",
    "            'edges': results_astar[2]['@@display_edge_set']\n",
    "        }\n",
    "\n",
    "        liste_to_undo.append(trajet_info)\n",
    "\n",
    "    # Step 6: Update the daily movement on each edge\n",
    "    edges_to_upsert = update_daily_movement(liste_to_undo)\n",
    "    \n",
    "\n",
    "\n",
    "    # Step 7: Upsert the updated edges back to the graph database\n",
    "\n",
    "    result = conn.upsertEdges(\n",
    "        sourceVertexType=\"Nodes\",\n",
    "        edgeType=\"distribute_to\",\n",
    "        targetVertexType=\"Nodes\",\n",
    "        edges=edges_to_upsert\n",
    "    )\n",
    "    print(f\"Number of edges updated :{result}, Edges updated :{edges_to_upsert}\")\n",
    "\n",
    "    return final_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges updated :3, Edges updated :[('Toulouse', 'Bilbao', {'Price': 5, 'Carga': 'Optimal', 'Capacity': 50, 'Daily_movement': 0}), ('Paris', 'Clermont-Ferrand', {'Price': 3, 'Carga': 'Sobrecargado', 'Capacity': 31, 'Daily_movement': 0}), ('Clermont-Ferrand', 'Toulouse', {'Price': 7, 'Carga': 'Sobrecargado', 'Capacity': 29, 'Daily_movement': 0})]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of main function\n",
    "csv_path = r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\CSV_entrega\\entrega_demo_test.csv\"\n",
    "final_df = main(entregas_and_paths, aristas_sobrecargadas, csv_path, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODE_Origin</th>\n",
       "      <th>CODE_Destination</th>\n",
       "      <th>Palets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Bilbao</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Bilbao</td>\n",
       "      <td>Paris</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID CODE_Origin CODE_Destination  Palets\n",
       "0   2      Bilbao            Paris       4\n",
       "2   1      Bilbao            Paris      40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfert_nodes_and_edges_reentrega_sequential(node_initial, node_final, charge, weight_attribute=\"Price\", nodetype=\"Nodes\", edgetype=\"distribute_to\"):\n",
    "    global palets_cost, log_list , entregas_and_paths\n",
    "    edge_to_restore = []\n",
    "    # Lanza la función A* para encontrar el camino óptimo entre node_initial y node_final\n",
    "    results_astar = conn.runInstalledQuery(\"tg_astar\", params={\n",
    "        \"source_vertex\": node_final, \"source_vertex.type\": nodetype,\n",
    "        \"target_vertex\": node_initial, \"target_vertex.type\": nodetype,\n",
    "        \"e_type_set\": edgetype, \"weight_type\": \"FLOAT\",\n",
    "        \"latitude\": \"latitude\", \"longitude\": \"longitude\",\n",
    "        \"weight_attribute\": weight_attribute,\n",
    "        \"print_stats\": \"True\"\n",
    "    })\n",
    "\n",
    "\n",
    "    key_csv_entrega = f\"{node_initial}_{node_final}_{charge}\"\n",
    "\n",
    "    for element in results_astar[2]['@@display_edge_set']:\n",
    "        # if element[\"attributes\"][\"Carga\"] == \"Sobrecargado\":\n",
    "        if charge + element[\"attributes\"][\"Daily_movement\"] > element[\"attributes\"][\"Capacity\"]:\n",
    "            edge_to_restore.append(element)\n",
    "            erase_edge(conn,element[\"to_id\"],element[\"from_id\"])\n",
    "\n",
    "    # if not edge_to_restore:\n",
    "    #     return\n",
    "\n",
    "    results_astar = conn.runInstalledQuery(\"tg_astar\", params={\n",
    "            \"source_vertex\": node_final, \"source_vertex.type\": nodetype,\n",
    "            \"target_vertex\": node_initial, \"target_vertex.type\": nodetype,\n",
    "            \"e_type_set\": edgetype, \"weight_type\": \"FLOAT\",\n",
    "            \"latitude\": \"latitude\", \"longitude\": \"longitude\",\n",
    "            \"weight_attribute\": weight_attribute,\n",
    "            \"print_stats\": \"True\"\n",
    "        })\n",
    "    \n",
    "    edges_to_upsert = [\n",
    "    (edge['from_id'], edge['to_id'], edge['attributes']) for edge in edge_to_restore\n",
    "    ]\n",
    "\n",
    "    conn.upsertEdges(\n",
    "    sourceVertexType=\"Nodes\",\n",
    "    edgeType=\"distribute_to\",\n",
    "    targetVertexType=\"Nodes\",\n",
    "    edges=edges_to_upsert\n",
    "    )\n",
    "\n",
    "\n",
    "    # Obtiene el orden de los nodos en el camino encontrado\n",
    "    order_taken = path_taken(results_astar)\n",
    "    \n",
    "    # Obtiene el conjunto de aristas involucradas en el camino\n",
    "    edge_set = results_astar[2]['@@display_edge_set']\n",
    "    \n",
    "    # Reorganiza las aristas de acuerdo al orden de los nodos en el camino\n",
    "    reordered_edges_set = []\n",
    "    for location in order_taken:\n",
    "        for edge in edge_set:\n",
    "            if edge['from_id'] == location:\n",
    "                reordered_edges_set.append(edge)\n",
    "\n",
    "    # Obtiene el conjunto de nodos involucrados en el camino\n",
    "    node_set = results_astar[2]['tmp']\n",
    "    \n",
    "    # Reorganiza los nodos de acuerdo al orden en el camino\n",
    "    reordered_nodes_set = []\n",
    "    for location in order_taken:\n",
    "        for item in node_set:\n",
    "            if item['v_id'] == location:\n",
    "                reordered_nodes_set.append(item)\n",
    "    entregas_and_paths[key_csv_entrega] = reordered_nodes_set \n",
    "\n",
    "\n",
    "       # Guardamos todos los paths to check which path can give problem de sobrecarga . \n",
    "    try:\n",
    "        # Recupera los atributos de los nodos inicial y final\n",
    "        node_i_stock = reordered_nodes_set[0][\"attributes\"][\"Stock\"]\n",
    "        node_f_stock = reordered_nodes_set[-1][\"attributes\"][\"Stock\"]\n",
    "        node_i_unload_capacity = reordered_nodes_set[0][\"attributes\"][\"UnloadCapacity\"]\n",
    "        node_f_load_capacity = reordered_nodes_set[-1][\"attributes\"][\"LoadCapacity\"]\n",
    "\n",
    "        # Verifica si hay suficiente inventario en el nodo inicial\n",
    "        with lock:\n",
    "            if charge > node_i_stock:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente inventario ({charge} > {node_i_stock})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Verifica si hay suficiente capacidad de carga en el nodo final\n",
    "            if charge > node_f_load_capacity:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de carga en el almacén de recepción (charge: {charge} > LoadCapacity: {node_f_load_capacity})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Verifica si hay suficiente capacidad de descarga en el nodo inicial\n",
    "            if charge > node_i_unload_capacity:\n",
    "                error_message = f\"{node_initial} hasta {node_final}: No hay suficiente capacidad de descarga desde el almacén de salida (charge: {charge} > UnloadCapacity: {node_i_unload_capacity})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "            # Recupera la capacidad de la arista y el movimiento diario actual\n",
    "            capacity_edge = reordered_edges_set[0][\"attributes\"][\"Capacity\"]\n",
    "            daily_movement = reordered_edges_set[0][\"attributes\"][\"Daily_movement\"]\n",
    "\n",
    "            # Verifica si la arista puede soportar el movimiento adicional de carga\n",
    "            if daily_movement + charge > capacity_edge:\n",
    "                error_message = f\"Enviar {charge} palets desde {node_initial} hasta {node_final}: Capacidad de la arista insuficiente ({daily_movement + charge} > {capacity_edge})\\n\"\n",
    "                log_list += error_message\n",
    "\n",
    "        # Actualiza los inventarios de los nodos inicial y final\n",
    "        nodes_batch = [\n",
    "            (node_initial, {\"Stock\": node_i_stock - charge}),\n",
    "            (node_final, {\"Stock\": node_f_stock + charge})\n",
    "        ]\n",
    "\n",
    "        # Actualiza el movimiento diario en las aristas recorridas y acumula el costo\n",
    "        edges_batch = []\n",
    "        with lock : \n",
    "            for element in reordered_edges_set:\n",
    "                element['attributes']['Daily_movement'] += charge\n",
    "                edges_batch.append((element[\"from_id\"], element[\"to_id\"], {\"Daily_movement\": element[\"attributes\"][\"Daily_movement\"]}))\n",
    "                palets_cost += element[\"attributes\"][\"Price\"]\n",
    "\n",
    "        # Añade los nodos inicial y final a los conjuntos de nodos únicos\n",
    "        unique_origin_nodes.add(node_initial)\n",
    "        unique_destination_nodes.add(node_final)\n",
    "\n",
    "        return nodes_batch , edges_batch \n",
    "\n",
    "    # Manejo de excepciones en caso de que ocurra algún error durante el proceso\n",
    "    except Exception as e:\n",
    "        log_list += f\"Error durante la transferencia de {node_initial} a {node_final}: {str(e)}\\n\"\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error en la transferancia: {e}\\n\"\n",
    "        log_list += error_message\n",
    "        return error_message\n",
    "\n",
    "\n",
    "# Thread safety\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def make_daily_movements_sequential_sobrecarga(input_file, column_origin, column_destination, column_transfert):\n",
    "    # Variables globales para rastrear estadísticas y almacenar datos.\n",
    "    global aristas_sobrecargadas,entregas_and_paths,operation_count, palets_number, palets_cost, unique_origin_nodes, unique_destination_nodes, log_list, nodes_batch_full, edges_batch_full, df\n",
    "    operation_count = 0  # Conteo de operaciones realizadas.\n",
    "    palets_number = 0  # Número total de palets transferidos.\n",
    "    palets_cost = 0  # Costo total asociado a los palets transferidos.\n",
    "    unique_origin_nodes = set()  # Conjunto de nodos de origen únicos.\n",
    "    unique_destination_nodes = set()  # Conjunto de nodos de destino únicos.\n",
    "    log_list = \" \"  # Registro de errores o mensajes importantes.\n",
    "    nodes_batch_full = []  # Lista para almacenar nodos actualizados.\n",
    "    edges_batch_full = []  # Lista para almacenar aristas actualizadas.\n",
    "    aristas_sobrecargadas = []\n",
    "    entregas_and_paths = {}\n",
    "\n",
    "    # Lee el archivo CSV y carga los datos en un DataFrame.\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Función para manejar la transferencia de volumen entre nodos.\n",
    "    def perform_transfer(row):\n",
    "        global log_list,operation_count, palets_number\n",
    "        try:\n",
    "            # Obtiene el origen, destino y volumen desde la fila del DataFrame.\n",
    "            origin = str(row[column_origin])\n",
    "            destination = str(row[column_destination])\n",
    "            volume = float(row[column_transfert])\n",
    "            \n",
    "            # Llama a la función para obtener nodos y aristas actualizados.\n",
    "            nodes_batch, edges_batch = transfert_nodes_and_edges_reentrega_sequential(origin, destination, volume)\n",
    "            \n",
    "            # Usa un lock para garantizar que las actualizaciones a las listas sean seguras en un entorno multihilo.\n",
    "            with lock:\n",
    "                # Actualiza o agrega nodos y aristas a las listas correspondientes.\n",
    "                # add_or_update_final_nodes_batch(nodes_batch_full, nodes_batch, volume)\n",
    "                add_or_update_final_edges_batch(edges_batch_full, edges_batch, volume)\n",
    "            \n",
    "            # Actualiza las métricas globales.\n",
    "            operation_count += 1  # Incrementa el conteo de operaciones.\n",
    "            palets_number += volume  # Incrementa el número total de palets.\n",
    "            unique_origin_nodes.add(origin)  # Agrega el nodo de origen al conjunto de nodos únicos.\n",
    "            unique_destination_nodes.add(destination)  # Agrega el nodo de destino al conjunto de nodos únicos.\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Registra un mensaje de error si ocurre una excepción.\n",
    "            print(e)\n",
    "            error_message = f\"{origin} hasta {destination}, no camino posible.\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        actualize_graph_carga(conn)\n",
    "        perform_transfer(row)\n",
    "\n",
    "    # Actualiza los nodos y aristas en la base de datos.\n",
    "    conn.upsertVertices(\"Nodes\", nodes_batch_full)\n",
    "    nodetype = \"Nodes\"\n",
    "    edgetype = \"distribute_to\"\n",
    "    conn.upsertEdges(sourceVertexType=nodetype, targetVertexType=nodetype, edgeType=edgetype, edges=edges_batch_full)\n",
    "    \n",
    "    # Llama a la función para actualizar el gráfico con el estado actual y obtiene el estado.\n",
    "    state_log = actualize_graph_carga(conn)\n",
    "\n",
    "    # Si no hubo errores, actualiza el mensaje de log para indicar éxito.\n",
    "    if log_list == \" \":\n",
    "        log_list = \"All operations were successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"lecsv_test.csv\", index=False)\n",
    "csv = r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\Final_version\\lecsv_test.csv\"\n",
    "make_daily_movements_sequential_sobrecarga(csv,\"CODE_Origin\", \"CODE_Destination\", \"Palets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de modificacion de red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io', port=9000): Max retries exceeded with url: /version (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015D1E59DB20>: Failed to resolve 'ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000015D1E59DB20>: Failed to resolve 'ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io', port=9000): Max retries exceeded with url: /version (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015D1E59DB20>: Failed to resolve 'ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m attributes_nodes_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m attributes_edges_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 64\u001b[0m conex_nodes_list \u001b[38;5;241m=\u001b[39m \u001b[43mconn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVertexAttrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m conex_edges_list \u001b[38;5;241m=\u001b[39m conn1\u001b[38;5;241m.\u001b[39mgetEdgeAttrs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistribute_to\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m conex_nodes_list:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphVertex.py:69\u001b[0m, in \u001b[0;36mpyTigerGraphVertex.getVertexAttrs\u001b[1;34m(self, vertexType)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m---> 69\u001b[0m et \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVertexType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertexType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m at \u001b[38;5;129;01min\u001b[39;00m et[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributes\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphVertex.py:98\u001b[0m, in \u001b[0;36mpyTigerGraphVertex.getVertexType\u001b[1;34m(self, vertexType, force)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m     96\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVertexTypes\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m vertexType:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphSchema.py:127\u001b[0m, in \u001b[0;36mpyTigerGraphSchema.getSchema\u001b[1;34m(self, udts, force)\u001b[0m\n\u001b[0;32m    124\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_versionGreaterThan4_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgsUrl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gsql/v1/schema/graphs/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphname,\n\u001b[0;32m    129\u001b[0m             authMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpwd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:606\u001b[0m, in \u001b[0;36mpyTigerGraphBase._versionGreaterThan4_0\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_versionGreaterThan4_0\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets if the TigerGraph database version is greater than 4.0 using gerVer().\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m        Boolean of whether databse version is greater than 4.0.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:583\u001b[0m, in \u001b[0;36mpyTigerGraphBase.getVer\u001b[1;34m(self, component, full)\u001b[0m\n\u001b[0;32m    580\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[0;32m    582\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m component\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m    585\u001b[0m         ret \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:543\u001b[0m, in \u001b[0;36mpyTigerGraphBase.getVersion\u001b[1;34m(self, raw)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    542\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m--> 543\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestppUrl\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrictJson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:417\u001b[0m, in \u001b[0;36mpyTigerGraphBase._get\u001b[1;34m(self, url, authMode, headers, resKey, skipCheck, params, strictJson)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    415\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locals(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m--> 417\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_req\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthMode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipCheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrictJson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    420\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(res))\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\pyTigerGraph\\pyTigerGraphBase.py:342\u001b[0m, in \u001b[0;36mpyTigerGraphBase._req\u001b[1;34m(self, method, url, authMode, headers, data, resKey, skipCheck, params, strictJson, jsonData, jsonResponse)\u001b[0m\n\u001b[0;32m    340\u001b[0m     res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(method, url, headers\u001b[38;5;241m=\u001b[39m_headers, json\u001b[38;5;241m=\u001b[39m_data, params\u001b[38;5;241m=\u001b[39mparams, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     res\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\.venv\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io', port=9000): Max retries exceeded with url: /version (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015D1E59DB20>: Failed to resolve 'ce7f96421a044abd9e8751d76a98bb8b.i.tgcloud.io' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import json\n",
    "global logs_modif_network\n",
    "logs_modif_network = \" \"\n",
    "\n",
    "\n",
    "# ADD NODES \n",
    "\n",
    "def add_node(id, attributes_dict=None, vertex_type=\"Nodes\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertVertex(vertex_type, id)\n",
    "        message =  f\"Nodo {id} añadido\\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        conn1.upsertVertex(vertex_type, id, attributes=attributes)\n",
    "        message =  f\"Nodo {id} añadido with attributos : {attributes} \\n\"\n",
    "        logs_modif_network +=message\n",
    "    return logs_modif_network\n",
    "\n",
    "def get_values_nodes(*args):\n",
    "    values = {key: val for key, val in zip(atributes_nodes.keys(), args)}\n",
    "    return values\n",
    "\n",
    "def get_values_edges(*args):\n",
    "    values = {key: val for key, val in zip(atributes_edges.keys(), args)}\n",
    "    return values[\"Capacity\"]\n",
    "\n",
    "\n",
    "def add_node_with_values(id, *args):\n",
    "    attributes = get_values_nodes(*args)\n",
    "    attributes_json = json.dumps(attributes)\n",
    "    actualize_graph_carga(conn)\n",
    "    return add_node(id, attributes_json), reset_html1()\n",
    "\n",
    "\n",
    "\n",
    "# ADD EDGES \n",
    "def add_new_edge(source_vertex_id, target_vertex_id, attributes_dict=None,source_vertex_type=\"Nodes\",target_vertex_type = \"Nodes\", edge_type=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertEdge(source_vertex_type, source_vertex_id, edge_type, target_vertex_type, target_vertex_id)\n",
    "        actualize_graph_carga(conn)\n",
    "        message =  f\"Arista de {source_vertex_id} hasta {target_vertex_id}  añadida\"\n",
    "        logs_modif_network += message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        actualize_graph_carga(conn)\n",
    "        conn1.upsertEdge(source_vertex_type, source_vertex_id, edge_type, target_vertex_type, target_vertex_id, attributes=attributes)\n",
    "        message =  f\"Arista de {source_vertex_id} hasta {target_vertex_id} añadida con los atributos siguientes: {attributes} \\n\"\n",
    "        logs_modif_network += message\n",
    "    return logs_modif_network\n",
    "\n",
    "def add_edge_with_values(source_vertex_id, target_vertex_id, *args):\n",
    "    attributes = get_values_edges(*args)\n",
    "    attributes_json = json.dumps(attributes)\n",
    "    return add_new_edge(source_vertex_id,target_vertex_id, attributes_json), reset_html1()\n",
    "\n",
    "# GET ATTRIBUTES (for front, it takes the list of atributes to create a box for each atribute)\n",
    "attributes_nodes_list = []\n",
    "attributes_edges_list = []\n",
    "conex_nodes_list = conn1.getVertexAttrs('Nodes')\n",
    "conex_edges_list = conn1.getEdgeAttrs('distribute_to')\n",
    "\n",
    "for node in conex_nodes_list:\n",
    "    attributes_nodes_list.append(node[0])\n",
    "\n",
    "\n",
    "for edge in conex_edges_list:\n",
    "    attributes_edges_list.append(edge[0])\n",
    "\n",
    "\n",
    "#Remove carga atributes as it is here only for a cheat with colors of the tigergraph map. no need to change that\n",
    "if 'Carga' in attributes_nodes_list:\n",
    "    attributes_nodes_list.remove('Carga')\n",
    "\n",
    "if 'Carga' in attributes_edges_list:\n",
    "    attributes_edges_list.remove('Carga')\n",
    "\n",
    "\n",
    "# GET NODES NAMES\n",
    "list_name_nodes = []\n",
    "name_nodes_conn = conn1.getVertices(\"Nodes\")\n",
    "for i in range(0, len(name_nodes_conn)):\n",
    "    list_name_nodes.append(name_nodes_conn[i][\"v_id\"])\n",
    "\n",
    "def add_edge(source_vertex_id, target_vertex_id, attributes_dict=None, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    if attributes_dict is None:\n",
    "        conn1.upsertEdge(sourceVertexType=vertex_type,\n",
    "                        targetVertexType=vertex_type,\n",
    "                        edgeType=edgeType,\n",
    "                        sourceVertexId=source_vertex_id,\n",
    "                        targetVertexId=target_vertex_id)\n",
    "        message =  f\"Arista desde {source_vertex_id} hasta {target_vertex_id} añadido\\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "    else:\n",
    "        attributes = json.loads(attributes_dict)\n",
    "        conn1.upsertEdge(sourceVertexType=vertex_type,\n",
    "                        targetVertexType=vertex_type,\n",
    "                        edgeType=edgeType,\n",
    "                        sourceVertexId=source_vertex_id,\n",
    "                        targetVertexId=target_vertex_id,\n",
    "                        attributes=attributes)\n",
    "        message =  f\"Arista {source_vertex_id} hasta {target_vertex_id} añadido con los atributos siguientes : {attributes} \\n\"\n",
    "        logs_modif_network +=message\n",
    "        return logs_modif_network\n",
    "\n",
    "\n",
    "#Erase Data\n",
    "\n",
    "def erase_edge(source_vertex_id, target_vertex_id, vertex_type=\"Nodes\", edgeType=\"distribute_to\"):\n",
    "    global logs_modif_network\n",
    "    result = conn1.delEdges(sourceVertexType=vertex_type,\n",
    "                           targetVertexType=vertex_type,\n",
    "                           edgeType=edgeType,\n",
    "                           sourceVertexId=source_vertex_id,\n",
    "                           targetVertexId=target_vertex_id)\n",
    "    \n",
    "    if result == {edgeType: 0}:\n",
    "        message = f\"Arista de {source_vertex_id} hacia {target_vertex_id} no existe o no ha podido borrarse\\n\"\n",
    "    else:\n",
    "        message = f\"Arista de  {source_vertex_id} hacia {target_vertex_id} borrada \\n\"\n",
    "    \n",
    "    logs_modif_network += message\n",
    "    return logs_modif_network , reset_html1()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def erase_node(id, vertex_type=\"Nodes\"):\n",
    "    global logs_modif_network\n",
    "    result = conn1.delVerticesById(vertexIds=id, vertexType=vertex_type)\n",
    "    \n",
    "    if result == 0:\n",
    "        message = f\"Nodo {id} no existe o no ha podido borrarse\\n\"\n",
    "    else:\n",
    "        message = f\"Nodo {id} borrado\\n\"\n",
    "    \n",
    "    logs_modif_network += message\n",
    "    return logs_modif_network , reset_html1()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add_edge_with_values(\"Las Palmas de Gran Canaria\",\"Seville\",{\"Capacity\": 0, \"Daily_movement\": 0, \"Price\": 0})\n",
    "erase_edge(\"Seville\",\"Madrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tab modif cargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_charge(conn, percentage):\n",
    "    aristas = conn.getEdgesByType(edgeType=\"distribute_to\")\n",
    "    new_charges = []\n",
    "    for arista in aristas:\n",
    "        current_charge = arista[\"attributes\"][\"Daily_movement\"]\n",
    "        new_charge = current_charge * (1 + percentage / 100)\n",
    "        arista[\"attributes\"][\"Daily_movement\"] = new_charge\n",
    "        new_charges.append((arista[\"from_id\"], arista[\"to_id\"], {\"Daily_movement\": new_charge}))\n",
    "\n",
    "    return new_charges\n",
    "\n",
    "\n",
    "def decrease_charge(conn, percentage):\n",
    "    aristas = conn.getEdgesByType(edgeType=\"distribute_to\")\n",
    "    new_charges = []\n",
    "    logs_state_aristas = \"\"\n",
    "    arista_counts = {\"Sobrecargado\": 0, \"Subcargado\": 0, \"Optimal\": 0}\n",
    "\n",
    "    for arista in aristas:\n",
    "        current_charge = arista[\"attributes\"][\"Daily_movement\"]\n",
    "        capacity_edge = arista[\"attributes\"][\"Capacity\"]\n",
    "        new_charge = current_charge * (1 - percentage / 100)\n",
    "        arista[\"attributes\"][\"Daily_movement\"] = new_charge\n",
    "        \n",
    "        if capacity_edge == 0:\n",
    "            nivel_arista = \"No definido\"\n",
    "        else:\n",
    "            if (new_charge / capacity_edge) > 1:\n",
    "                nivel_arista = \"Sobrecargado\"\n",
    "                arista_counts[\"Sobrecargado\"] += 1\n",
    "                logs_state_aristas += f\"⚠️ La arista {arista['from_id']} hasta {arista['to_id']} es {nivel_arista} con un movimiento diario de {new_charge}\\n\"\n",
    "            elif 0.7 < (new_charge / capacity_edge) <= 1:\n",
    "                nivel_arista = \"Optimal\"\n",
    "                arista_counts[\"Optimal\"] += 1\n",
    "            else:\n",
    "                nivel_arista = \"Subcargado\"\n",
    "                arista_counts[\"Subcargado\"] += 1\n",
    "\n",
    "        new_charges.append((arista[\"from_id\"], arista[\"to_id\"], {\"Daily_movement\": new_charge, \"Carga\": nivel_arista}))\n",
    "\n",
    "    if logs_state_aristas == \"\":\n",
    "        logs_state_aristas = \"Aucune arista n'est surchargée.\"\n",
    "\n",
    "    results = {\n",
    "        \"logs\": logs_state_aristas,\n",
    "        \"arista_counts\": arista_counts,\n",
    "    }\n",
    "\n",
    "    return results , new_charges\n",
    "\n",
    "def update_batch_increase(conn,percentage):\n",
    "    batch = increase_charge(conn, percentage)\n",
    "    conn.upsertEdges(sourceVertexType=\"Nodes\", targetVertexType=\"Nodes\", edgeType=\"distribute_to\", edges=batch)\n",
    "\n",
    "def update_batch_decrease(conn,percentage):\n",
    "    logs , batch = decrease_charge(conn, percentage)\n",
    "    conn.upsertEdges(sourceVertexType=\"Nodes\", targetVertexType=\"Nodes\", edgeType=\"distribute_to\", edges=batch)\n",
    "\n",
    "    # return(gr.Textbox(value=logs[\"logs\"], label=\"Estado de la red\", visible=True,lines=2,max_lines=10, elem_classes=\"dropdown\"),\n",
    "    #     gr.Textbox(value=logs[\"node_counts\"][\"Sobrecargado\"], label=\"Nodos sobrecargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "    #     gr.Textbox(value=logs[\"node_counts\"][\"Subcargado\"], label=\"Nodos subcargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "    #     gr.Textbox(value=int(round(logs[\"arista_counts\"][\"Sobrecargado\"]/2,0)), label=\"Aristas sobrecargadas\", visible=True, elem_classes=\"dropdown\"),  #Divide by 2 to take into account the fact that the edges are bidirectional\n",
    "    #     gr.Textbox(value=int(round(logs[\"arista_counts\"][\"Subcargado\"]/2,0)), label=\"Aristas subcargadas\", visible=True, elem_classes=\"dropdown\"))\n",
    "\n",
    "\n",
    "\n",
    "# a = update_batch_decrease(conn,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_daily_movements_with_percentages_positive(input_file, column_origin, column_destination, column_transfert,percentage):\n",
    "    global operation_count3, palets_number3, palets_cost3, unique_origin_nodes, unique_destination_nodes, log_list, nodes_batch_full2,edges_batch_full2,df\n",
    "    operation_count3 = 0\n",
    "    palets_number3 = 0\n",
    "    palets_cost3 = 0\n",
    "    unique_origin_nodes = set()\n",
    "    unique_destination_nodes = set()\n",
    "    log_list = \" \"\n",
    "    nodes_batch_full2 = []\n",
    "    edges_batch_full2 = []\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    def perform_transfer(row):\n",
    "        global log_list\n",
    "        try:\n",
    "            origin = str(row[column_origin])\n",
    "            destination = str(row[column_destination])\n",
    "            positive_or_negative = \"positive\"\n",
    "            if positive_or_negative == \"positive\":\n",
    "                volume = float(row[column_transfert]) * (percentage / 100)\n",
    "            if positive_or_negative == \"negative\":\n",
    "                volume = float(row[column_transfert]) * (1 - percentage / 100)\n",
    "            nodes_batch, edges_batch = transfert_nodes_and_edges1(origin, destination, volume)\n",
    "            with lock:\n",
    "                add_or_update_final_nodes_batch(nodes_batch_full2, nodes_batch, volume)\n",
    "                add_or_update_final_edges_batch(edges_batch_full2, edges_batch, volume)\n",
    "        except Exception as e:\n",
    "            error_message = f\"{origin} hasta {destination}, no camino posible.\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1000) as executor:\n",
    "        futures = [executor.submit(perform_transfer, row) for index, row in df.iterrows()]\n",
    "        for future in as_completed(futures):\n",
    "            pass\n",
    "\n",
    "    conn1.upsertVertices(\"Nodes\", nodes_batch_full2)\n",
    "    nodetype = \"Nodes\"\n",
    "    edgetype = \"distribute_to\"\n",
    "    conn1.upsertEdges(sourceVertexType=nodetype, targetVertexType=nodetype, edgeType=edgetype, edges=edges_batch_full2)\n",
    "    results2 = actualize_graph_carga(conn1)\n",
    "\n",
    "    if log_list == \" \":\n",
    "        log_list = \"All operations were successful.\"\n",
    "\n",
    "    return (reset_html1(),\n",
    "        gr.Textbox(value=results2[\"logs\"], label=\"Estado de la red\", visible=True,lines=2,max_lines=10, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=results2[\"node_counts\"][\"Sobrecargado\"], label=\"Nodos sobrecargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=results2[\"node_counts\"][\"Subcargado\"], label=\"Nodos subcargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=int(round(results2[\"arista_counts\"][\"Sobrecargado\"]/2,0)), label=\"Aristas sobrecargadas\", visible=True, elem_classes=\"dropdown\"),  #Divide by 2 to take into account the fact that the edges are bidirectional\n",
    "        gr.Textbox(value=int(round(results2[\"arista_counts\"][\"Subcargado\"]/2,0)), label=\"Aristas subcargadas\", visible=True, elem_classes=\"dropdown\")\n",
    "        )\n",
    "    # return (log_list, reset_html(), gr.Button(\"Recargar mapa\",elem_classes=\"otherbutton\",visible=True),\n",
    "    #     gr.Number(label=\"KPI : Numero de operaciones\", value = operation_count,visible=True,elem_classes=\"dropdown\"), \n",
    "    #     gr.Number(label=\"KPI : Numero de palets\",value = palets_number, visible=True,elem_classes=\"dropdown\"), \n",
    "    #     gr.Number(label=\"KPI : Numero de origen\",value = len(unique_origin_nodes), visible=True,elem_classes=\"dropdown\"), \n",
    "    #     gr.Number(label=\"KPI : Destinaciones diferentes\",value = len(unique_destination_nodes), visible=True,elem_classes=\"dropdown\"), \n",
    "    #     gr.Number(label=\"KPI : Coste total\",value = palets_cost, visible=True,elem_classes=\"dropdown\"), \n",
    "    #     gr.Number(label=\"KPI : Coste por palet\",value = round(palets_cost/palets_number,1), visible=True,elem_classes=\"dropdown\"))\n",
    "\n",
    "def make_daily_movements_with_percentages_negative(input_file, column_origin, column_destination, column_transfert,percentage):\n",
    "    global operation_count3, palets_number3, palets_cost3, unique_origin_nodes, unique_destination_nodes, log_list, nodes_batch_full2,edges_batch_full2,df\n",
    "    operation_count3 = 0\n",
    "    palets_number3 = 0\n",
    "    palets_cost3 = 0\n",
    "    unique_origin_nodes = set()\n",
    "    unique_destination_nodes = set()\n",
    "    log_list = \" \"\n",
    "    nodes_batch_full2 = []\n",
    "    edges_batch_full2 = []\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    def perform_transfer(row):\n",
    "        global log_list\n",
    "        try:\n",
    "            origin = str(row[column_origin])\n",
    "            destination = str(row[column_destination])\n",
    "            positive_or_negative = \"negative\"\n",
    "            if positive_or_negative == \"positive\":\n",
    "                volume = float(row[column_transfert]) * (percentage / 100)\n",
    "            if positive_or_negative == \"negative\":\n",
    "                volume = float(row[column_transfert]) * (1 - percentage / 100)\n",
    "            nodes_batch, edges_batch = transfert_nodes_and_edges1(origin, destination, volume)\n",
    "            with lock:\n",
    "                add_or_update_final_nodes_batch(nodes_batch_full2, nodes_batch, volume)\n",
    "                add_or_update_final_edges_batch(edges_batch_full2, edges_batch, volume)\n",
    "        except Exception as e:\n",
    "            error_message = f\"{origin} hasta {destination}, no camino posible.\\n\"\n",
    "            log_list += error_message\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1000) as executor:\n",
    "        futures = [executor.submit(perform_transfer, row) for index, row in df.iterrows()]\n",
    "        for future in as_completed(futures):\n",
    "            pass\n",
    "\n",
    "    conn1.upsertVertices(\"Nodes\", nodes_batch_full2)\n",
    "    nodetype = \"Nodes\"\n",
    "    edgetype = \"distribute_to\"\n",
    "    conn1.upsertEdges(sourceVertexType=nodetype, targetVertexType=nodetype, edgeType=edgetype, edges=edges_batch_full2)\n",
    "    results2 = actualize_graph_carga(conn1)\n",
    "\n",
    "    if log_list == \" \":\n",
    "        log_list = \"All operations were successful.\"\n",
    "\n",
    "    return (reset_html1(),\n",
    "        gr.Textbox(value=results2[\"logs\"], label=\"Estado de la red\", visible=True,lines=2,max_lines=10, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=results2[\"node_counts\"][\"Sobrecargado\"], label=\"Nodos sobrecargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=results2[\"node_counts\"][\"Subcargado\"], label=\"Nodos subcargados\", visible=True, elem_classes=\"dropdown\"),\n",
    "        gr.Textbox(value=int(round(results2[\"arista_counts\"][\"Sobrecargado\"]/2,0)), label=\"Aristas sobrecargadas\", visible=True, elem_classes=\"dropdown\"),  #Divide by 2 to take into account the fact that the edges are bidirectional\n",
    "        gr.Textbox(value=int(round(results2[\"arista_counts\"][\"Subcargado\"]/2,0)), label=\"Aristas subcargadas\", visible=True, elem_classes=\"dropdown\")\n",
    "        )\n",
    "\n",
    "# a = make_daily_movements_with_percentages(r\"C:\\Users\\JulienRigot\\OneDrive - LIS Data Solutions\\Escritorio\\code_GORDIAS\\base de datos graph\\Tigergraph\\CSV_entrega\\df_1_move.csv\", \"CODE_Origin\", \"CODE_Destination\", \"Palets\",10,\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_path(input_file):\n",
    "    df = pd.read_csv(input_file.name)\n",
    "    target_column = sorted(list(df.columns))\n",
    "    return  gr.Dropdown(choices = target_column) , gr.Dropdown(choices = target_column) , gr.Dropdown(choices = target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value first tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_edge = \"distribute_to\"\n",
    "name_node = \"Nodes\"\n",
    "\n",
    "\n",
    "#######RED ORIGINAL######\n",
    "\n",
    "numEdges = conn.getEdgeCount(name_edge)\n",
    "numNodes = conn.getVertexCount(name_node)\n",
    "edge = conn.getEdgeStats(name_edge)\n",
    "avg_capacity = round(edge[name_edge]['Capacity']['AVG'],1)\n",
    "\n",
    "edges = conn.getEdgesByType(name_edge)\n",
    "total_capacity_edges = 0\n",
    "for warehouse in edges :\n",
    "    capacity_edges = warehouse[\"attributes\"][\"Capacity\"]     \n",
    "    total_capacity_edges += round(capacity_edges,1)\n",
    "\n",
    "almacenes = conn.getVertices(name_node)\n",
    "total_capacity_warehouse = 0\n",
    "for warehouse in almacenes :\n",
    "    capacity_edges = warehouse[\"attributes\"][\"Capacity\"]      \n",
    "    total_capacity_warehouse += round(capacity_edges,1)\n",
    "    total_capacity_warehouse = round(total_capacity_warehouse,1)\n",
    "\n",
    "almacen = conn.getVertices(name_node)\n",
    "total_stock = 0\n",
    "for warehouse in almacen :\n",
    "    stock = warehouse[\"attributes\"][\"Stock\"]     \n",
    "    total_stock += stock\n",
    "    total_stock = round(total_stock,0)\n",
    "\n",
    "capacity_per_edge = round(total_capacity_warehouse/numEdges,1)\n",
    "\n",
    "\n",
    "#######RED SIMULADA######\n",
    "import concurrent.futures\n",
    "\n",
    "def get_num_edges(conn, edge_type):\n",
    "    return conn.getEdgeCount(edge_type)\n",
    "\n",
    "def get_num_nodes(conn, node_type):\n",
    "    return conn.getVertexCount(node_type)\n",
    "\n",
    "def get_avg_capacity(conn, edge_type):\n",
    "    edges = conn.getEdgeStats(edge_type)\n",
    "    avg_capacity = edges[edge_type]['Capacity']['AVG']\n",
    "    return round(avg_capacity/2, 1)\n",
    "\n",
    "def get_total_capacity_edges(conn, edge_type):\n",
    "    edges = conn.getEdgesByType(edge_type)\n",
    "    total_capacity = 0\n",
    "    for edge in edges:\n",
    "        total_capacity += edge[\"attributes\"][\"Capacity\"]\n",
    "    return total_capacity /2\n",
    "\n",
    "def get_total_capacity_nodes(conn, node_type):\n",
    "    warehouses = conn.getVertices(node_type)\n",
    "    total_capacity = 0\n",
    "    for warehouse in warehouses:\n",
    "        total_capacity += warehouse[\"attributes\"][\"Capacity\"]\n",
    "    return round(total_capacity, 1)\n",
    "\n",
    "def get_total_stock(conn, node_type):\n",
    "    warehouses = conn.getVertices(node_type)\n",
    "    total_stock = 0\n",
    "    for warehouse in warehouses:\n",
    "        total_stock += warehouse[\"attributes\"][\"Stock\"]\n",
    "    return round(total_stock, 0)\n",
    "\n",
    "def get_capacity_per_edge(total_capacity_warehouses, num_edges):\n",
    "    if num_edges == 0:\n",
    "        return 0\n",
    "    return round(total_capacity_warehouses / num_edges, 0)\n",
    "\n",
    "def get_KPI(conn, edge_type, node_type):\n",
    "     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            \"num_edges\": executor.submit(get_num_edges, conn, edge_type),\n",
    "            \"num_nodes\": executor.submit(get_num_nodes, conn, node_type),\n",
    "            \"avg_capacity\": executor.submit(get_avg_capacity, conn, edge_type),\n",
    "            \"total_capacity_edges\": executor.submit(get_total_capacity_edges, conn, edge_type),\n",
    "            \"total_capacity_warehouses\": executor.submit(get_total_capacity_nodes, conn, node_type),\n",
    "            \"total_stock\": executor.submit(get_total_stock, conn, node_type)\n",
    "        }\n",
    "\n",
    "        results = {name: future.result() for name, future in futures.items()}\n",
    "        results[\"capacity_per_edge\"] = get_capacity_per_edge(results[\"total_capacity_warehouses\"], results[\"num_edges\"])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JulienRigot\\Projets\\zzEnvs\\Transport\\Lib\\site-packages\\gradio\\routes.py:1150: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "c:\\Users\\JulienRigot\\Projets\\zzEnvs\\Transport\\Lib\\site-packages\\fastapi\\applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tigergraph_insights_map1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 229\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    228\u001b[0m     lis \u001b[38;5;241m=\u001b[39m GradioMetro()\n\u001b[1;32m--> 229\u001b[0m     \u001b[43mlis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfront_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m#Reactualizar los colores cuando se hace modif de arrista despues de la entrega\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m, in \u001b[0;36mGradioMetro.front_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mRow():\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mColumn(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     13\u001b[0m         map_tiger \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mHTML(value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m <html lang=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m><head><meta charset=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m><meta name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviewport\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m content=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth=device-width, \u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m                initial-scale=1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m></head><body><iframe width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1000\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m800\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;124m                src=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtigergraph_insights_map1\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m                title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtigergarph insights\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m frameborder=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m></iframe></body></html>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m         btn_reload_map \u001b[38;5;241m=\u001b[39m  gr\u001b[38;5;241m.\u001b[39mButton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecargar mapa\u001b[39m\u001b[38;5;124m\"\u001b[39m,elem_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124motherbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m         btn_reload_map\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mreset_html1_1000,outputs\u001b[38;5;241m=\u001b[39m[map_tiger])  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'tigergraph_insights_map1' is not defined"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "class GradioMetro:\n",
    "    \n",
    "    def front_func(self):\n",
    "        with gr.Blocks(css=\"stylessupersimple.css\", title =\"Red de distribución\",theme=\"Monochrome\") as demo:\n",
    "            gr.Markdown(\"## \"\"![](file/Gradio/logo_lis.svg) Red de distribución\"\"\", elem_classes=\"cabecero\")\n",
    "            with gr.Tabs() as tabs:\n",
    "\n",
    "                with gr.TabItem(\"KPIs red\",id=1):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=5):\n",
    "                            map_tiger = gr.HTML(value = \"\"\" <html lang=\"en\"><head><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, \n",
    "                                    initial-scale=1.0\"></head><body><iframe width=\"1000\" height=\"800\"\n",
    "                                    src=\"\"\" + tigergraph_insights_map1 + \"\"\" \n",
    "                                    title=\"tigergarph insights\" frameborder=\"0\"></iframe></body></html>\"\"\")\n",
    "                            btn_reload_map =  gr.Button(\"Recargar mapa\",elem_classes=\"otherbutton\")\n",
    "                            btn_reload_map.click(fn=reset_html1_1000,outputs=[map_tiger])  \n",
    "                        with gr.Column(scale=2):\n",
    "                            gr.Number(label=\"KPI : Cantidad de Aristas\",value=numEdges,elem_classes='textbox')\n",
    "                            gr.Number(label=\"KPI : Cantidad de Nodos\",value = numNodes,elem_classes='textbox')\n",
    "                            gr.Number(label= \"KPI : Capacidad media por arista\",value = avg_capacity,elem_classes='textbox')\n",
    "                            gr.Number(label=\"KPI : Capacidad total de las aristas \",value = total_capacity_edges,elem_classes='textbox')\n",
    "                            gr.Number(label= \"KPI : Capacidad total de los Almacenes\",value = total_capacity_warehouse,elem_classes='textbox')\n",
    "                            gr.Number(label=\"KPI : Stock total de los almacenes\",value = total_stock,elem_classes='textbox')\n",
    "                \n",
    "                with gr.TabItem(\"Cargar movimientos\",id=2):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=5):\n",
    "                            upload_file = gr.File(label=\"Cargar fichero\", type=\"filepath\",elem_classes='textbox')\n",
    "\n",
    "                        with gr.Column(scale=4):\n",
    "                            dropdown_column_start = gr.Dropdown(label='Columna del envio', choices = [],  interactive=True, allow_custom_value= True,elem_classes='textbox')\n",
    "                            dropdown_column_arrival = gr.Dropdown(label='Columna de la llegada', choices = [],  interactive=True, allow_custom_value= True,elem_classes='textbox')\n",
    "                            dropdown_column_quantity = gr.Dropdown(label='Columna de la cantidad', choices = [],  interactive=True, allow_custom_value= True ,elem_classes='textbox')\n",
    "                            load_data_send = gr.Button(\"Lanzar los envios\",elem_classes=\"otherbutton\")\n",
    "                            upload_file.upload(read_csv_path,inputs=[upload_file],outputs=[dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity])\n",
    "                    with gr.Row():\n",
    "                        logs = gr.Textbox(label=\"Logs\",elem_classes=\"dropdown\")\n",
    "\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=10):\n",
    "                            map_entrega = gr.HTML(value = \"\"\" <html lang=\"en\"><head><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, \n",
    "                                    initial-scale=1.0\"></head><body><iframe width=\"800\" height=\"800\"\n",
    "                                    src=\"\"\" + tigergraph_insights_map + \"\"\" \n",
    "                                    title=\"tigergarph insights\" frameborder=\"0\"></iframe></body></html>\"\"\", visible = False\n",
    "                                    )\n",
    "                            btn_reload_map2 = gr.Button(\"Recargar mapa\",elem_classes=\"otherbutton\",visible=False)\n",
    "                            btn_reload_map2.click(fn=reset_html,outputs=[map_entrega])  \n",
    "                        with gr.Column(scale=6):\n",
    "                  \n",
    "                            operation_number = gr.Number(label=\"KPI : Numero de operaciones\", visible=False)\n",
    "                            palets_number = gr.Number(label=\"KPI : Numero de palets\", visible=False)\n",
    "                            origin_number = gr.Number(label=\"KPI : Numero de origen\", visible=False)\n",
    "                            distinct_destinations = gr.Number(label=\"KPI : Destinaciones diferentes\", visible=False)\n",
    "                            total_cost = gr.Number(label=\"KPI : Coste total\", visible=False)\n",
    "                            total_cost_per_palet = gr.Number(label=\"KPI : Coste por palet\", visible=False)\n",
    "        \n",
    "                        load_data_send.click(make_daily_movements,inputs=[upload_file,dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity],\n",
    "                                             outputs=[logs,map_entrega,btn_reload_map2,operation_number,palets_number,origin_number,distinct_destinations,total_cost,total_cost_per_palet])\n",
    "\n",
    "                with gr.TabItem(\"Escenario What if\", id=3):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            gr.Markdown(\"# Añadir/modificar nodo\",elem_classes=\"cabecero\")\n",
    "                            new_node_id = gr.Textbox(label=\"Nombre nuevo nodo\",elem_classes=\"dropdown\")\n",
    "                            for e in attributes_nodes_list:\n",
    "                                with gr.Row():\n",
    "                                    atributes_nodes[e] = gr.Number(label=e,elem_classes=\"dropdown\")\n",
    "                        \n",
    "                            with gr.Row():\n",
    "                                btn_modif_nodes = gr.Button(\"Añadir/modificar nodos\",elem_classes=\"otherbutton\")\n",
    "\n",
    "                        with gr.Column():\n",
    "                            gr.Markdown(\"# Añadir/modificar arista\",elem_classes=\"cabecero\")\n",
    "\n",
    "                            source_vertex_id_edge = gr.Textbox(label=\"Nodo de origen\",elem_classes=\"dropdown\",elem_id = \"source_vertex\")\n",
    "                            target_vertex_id_edge = gr.Textbox(label=\"Nodo de destino\",elem_classes=\"dropdown\")\n",
    "\n",
    "                            for e in attributes_edges_list:\n",
    "                                with gr.Row():\n",
    "                                    atributes_edges[e] = gr.Number(label=e,elem_classes=\"dropdown\")\n",
    "                            btn_modif_edge = gr.Button(\"Añadir/modificar arista\",elem_classes=\"otherbutton\")\n",
    "\n",
    "                        with gr.Column():\n",
    "                            gr.Markdown(\"# Borrar nodo/arista\",elem_classes=\"cabecero\")\n",
    "                            source_vertex_id = gr.Textbox(label=\"Nodo de origen\",elem_classes=\"dropdown\",container = True)\n",
    "                            target_vertex_id = gr.Textbox(label=\"Nodo de destino (si es una arista)\",elem_classes=\"dropdown\",container = True)\n",
    "                            btn_erase_node = gr.Button(\"Borrar nodo\",elem_classes=\"otherbutton\")\n",
    "                            btn_erase_edge = gr.Button(\"Borrar arista\",elem_classes=\"otherbutton\")  \n",
    "\n",
    "                    with gr.Row():\n",
    "                        logs = gr.Textbox(label = \"logs\",elem_classes=\"dropdown\")\n",
    "\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=5):\n",
    "                            upload_file = gr.File(label=\"Cargar fichero\", type=\"filepath\",elem_classes='textbox')\n",
    "                        with gr.Column(scale=4):\n",
    "                            dropdown_column_start = gr.Dropdown(label='Columna del envio', choices = [],  interactive=True, allow_custom_value= True, elem_classes=\"dropdown\")\n",
    "                            dropdown_column_arrival = gr.Dropdown(label='Columna de la llegada', choices = [],  interactive=True, allow_custom_value= True, elem_classes=\"dropdown\")\n",
    "                            dropdown_column_quantity = gr.Dropdown(label='Columna de la cantidad', choices = [],  interactive=True, allow_custom_value= True, elem_classes=\"dropdown\")\n",
    "                            load_data_send1 = gr.Button(\"Lanzar los envios en la red modificada\",elem_classes=\"otherbutton\")\n",
    "                            upload_file.upload(read_csv_path,inputs=[upload_file],outputs=[dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity])\n",
    "\n",
    "                    with gr.Row():\n",
    "                        logs1 = gr.Textbox(label = \"logs\",elem_classes=\"dropdown\")\n",
    "\n",
    "                            \n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            markdown1 = gr.Markdown(\"# KPIs red original\",visible=False,elem_classes = \"cabecero\")\n",
    "                            num_edges_number1 = gr.Number(label=\"KPI : Cantidad de Aristas\",value=numEdges,visible=False)\n",
    "                            num_nodes_number1 = gr.Number(label=\"KPI : Cantidad de Nodos\",value = numNodes,visible=False)\n",
    "                            capacity_average_number1 = gr.Number(label= \"KPI : Capacidad media por arista\",value = avg_capacity,visible=False)\n",
    "                            total_capacity_edge_number1 = gr.Number(label=\"KPI : Capacidad total de las aristas \",value = total_capacity_edges,visible=False)\n",
    "                            total_capacity_warehouse_number1 = gr.Number(label= \"KPI : Capacidad total de los Almacenes\",value = total_capacity_warehouse,visible=False)\n",
    "                            total_stock_number1 = gr.Number(label=\"KPI : Stock total de los almacenes\",value = total_stock,visible=False)\n",
    "                            ####KPI ENTREGA RED ORGINAL####\n",
    "                        with gr.Column():\n",
    "                            markdown2 = gr.Markdown(\"# KPIs Entrega original\",visible=False,elem_classes = \"cabecero\")\n",
    "                            operation_number1 = gr.Number(label= \"KPI : Numero de operaciones\",visible=False)\n",
    "                            palets_number1 = gr.Number(label= \"KPI : Numero de palets\",visible=False)\n",
    "                            origin_number1 = gr.Number(label= \"KPI : Numero de origen\",visible=False)\n",
    "                            distinct_destinations1 = gr.Number(label= \"KPI : Destinaciones diferentes\",visible=False)\n",
    "                            total_cost1 = gr.Number(label= \"KPI : Coste total\",visible=False)\n",
    "                            total_cost_per_palet1 = gr.Number(label= \"KPI : Coste por palet\",visible=False)\n",
    "                        with gr.Column():\n",
    "                            ###KPI RED MODIFICADA####\n",
    "                            markdown1_simul =gr.Markdown(\"# KPIs red modificada\",visible=False,elem_classes = \"cabecero\")\n",
    "                            num_edges_number_simul = gr.Number(label=\"KPI : Cantidad de Aristas\",value=\"numEdges1\",visible=False)\n",
    "                            num_nodes_number_simul = gr.Number(label=\"KPI : Cantidad de Nodos\",value = \"numNodes1\",visible=False)\n",
    "                            capacity_average_number_simul = gr.Number(label= \"KPI : Capacidad media por arista\",value = \"avg_capacity1\",visible=False)\n",
    "                            total_capacity_edge_number_simul = gr.Number(label=\"KPI : Capacidad total de las aristas \",value = \"total_capacity_edges1\",visible=False)\n",
    "                            total_capacity_warehouse_number_simul = gr.Number(label= \"KPI : Capacidad total de los Almacenes\",value = \"total_capacity_warehouse1\",visible=False)\n",
    "                            total_stock_number_simul = gr.Number(label=\"KPI : Stock total de los almacenes\",value = \"total_stock1\",visible=False)\n",
    "                        with gr.Column():\n",
    "                            markdown2_simul =gr.Markdown(\"# KPIs entrega red modificada\",visible=False,elem_classes = \"cabecero\")\n",
    "                            operation_number_simul = gr.Number(label= \"KPI : Numero de operaciones\",visible=False)\n",
    "                            palets_number_simul = gr.Number(label= \"KPI : Numero de palets\",visible=False)\n",
    "                            origin_number_simul = gr.Number(label= \"KPI : Numero de origen\",visible=False)\n",
    "                            distinct_destinations_simul = gr.Number(label= \"KPI : Destinaciones diferentes\",visible=False)\n",
    "                            total_cost_simul = gr.Number(label= \"KPI : Coste total\",visible=False)\n",
    "                            total_cost_per_palet_simul = gr.Number(label= \"KPI : Coste por palet\",visible=False)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        diference_price = gr.Number(label=\" porcentaje diferencia precio\",value = avg_capacity,visible=False)\n",
    "\n",
    "\n",
    "\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            markdown_mapa_original =gr.Markdown(\"# Mapa red original\",visible=False,elem_classes = \"cabecero\")\n",
    "                            map_entrega_original = gr.HTML(value = \"\"\" <html lang=\"en\"><head><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, \n",
    "                                        initial-scale=1.0\"></head><body><iframe width=\"800\" height=\"800\"\n",
    "                                        src=\"\"\" + tigergraph_insights_map + \"\"\" \n",
    "                                        title=\"tigergarph insights\" frameborder=\"0\"></iframe></body></html>\"\"\",visible=False)\n",
    "                            btn_reload_map_original = gr.Button(\"Recargar mapa\",visible=False)\n",
    "\n",
    "                        with gr.Column():\n",
    "                            markdown_mapa_modificada =gr.Markdown(\"# Mapa modificada\",visible=False,elem_classes = \"cabecero\")\n",
    "                            map_entrega_simulacion = gr.HTML(value = \"\"\" <html lang=\"en\"><head><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, \n",
    "                                        initial-scale=1.0\"></head><body><iframe width=\"800\" height=\"800\"\n",
    "                                        src=\"\"\" + tigergraph_insights_map1 + \"\"\" \n",
    "                                        title=\"tigergarph insights\" frameborder=\"0\"></iframe></body></html>\"\"\",visible=False)\n",
    "                            btn_reload_map_simul = gr.Button(\"Recargar mapa\",visible=False)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        valor_porcentaje = gr.Number(label=\"Porcentaje de subir/bajar la carga\",visible=False)\n",
    "                        \n",
    "                    with gr.Row():\n",
    "                        btn_subir_carga = gr.Button(\"Subir carga\",elem_classes=\"otherbutton\",visible=False)\n",
    "                        btn_bajar_carga = gr.Button(\"Bajar carga\",elem_classes=\"otherbutton\",visible=False)\n",
    "\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=5):\n",
    "                            logs_red_state_o = gr.Textbox(label=\"Estado de la red\", visible=False,elem_classes=\"dropdown\")\n",
    "\n",
    "                            KPIo_node_sobrecargado = gr.Textbox(label=\"Nodos sobrecargados\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIo_node_subcargado = gr.Textbox(label=\"Nodos subcargados\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIo_arista_sobrecargada = gr.Textbox(label=\"Aristas sobrecargadas\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIo_arista_subcargada = gr.Textbox(label=\"Aristas subcargadas\", visible=False,elem_classes=\"dropdown\")\n",
    "                        with gr.Column(scale=5):\n",
    "                            logs_red_state_m = gr.Textbox(label=\"Estado de la red\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIm_node_sobrecargado = gr.Textbox(label=\"Nodos sobrecargados\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIm_node_subcargado = gr.Textbox(label=\"Nodos subcargados\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIm_arista_sobrecargada = gr.Textbox(label=\"Aristas sobrecargadas\", visible=False,elem_classes=\"dropdown\")\n",
    "                            KPIm_arista_subcargada = gr.Textbox(label=\"Aristas subcargadas\", visible=False,elem_classes=\"dropdown\")\n",
    "\n",
    "\n",
    "                        load_data_send1.click(make_daily_movements1,inputs=[upload_file,dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity],\n",
    "                                            outputs=[logs1,markdown_mapa_original,map_entrega_original,btn_reload_map_original,markdown_mapa_modificada,map_entrega_simulacion,btn_reload_map_simul,\n",
    "                                                    markdown1,num_edges_number1,num_nodes_number1,capacity_average_number1,total_capacity_edge_number1,total_capacity_warehouse_number1,total_stock_number1,   #Red original\n",
    "                                                    markdown2,operation_number1,palets_number1,origin_number1,distinct_destinations1,total_cost1,total_cost_per_palet1,   #Entrega original\n",
    "                                                    markdown1_simul,num_edges_number_simul,num_nodes_number_simul,capacity_average_number_simul,total_capacity_edge_number_simul,total_capacity_warehouse_number_simul,total_stock_number_simul, #Red simulada\n",
    "                                                    markdown2_simul,operation_number_simul,palets_number_simul,origin_number_simul,distinct_destinations_simul,total_cost_simul,total_cost_per_palet_simul #Entrega simulada \n",
    "                                                    ,logs_red_state_o,\n",
    "                                                    KPIo_node_sobrecargado,KPIo_node_subcargado,KPIo_arista_sobrecargada,KPIo_arista_subcargada,\n",
    "                                                    logs_red_state_m,\n",
    "                                                    KPIm_node_sobrecargado,KPIm_node_subcargado,KPIm_arista_sobrecargada,KPIm_arista_subcargada,diference_price,\n",
    "                                                    valor_porcentaje,btn_subir_carga,btn_bajar_carga],\n",
    "                                                    scroll_to_output=True) \n",
    "                        \n",
    "                        btn_subir_carga.click(make_daily_movements_with_percentages_positive,inputs=[upload_file,dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity,valor_porcentaje],\n",
    "                                              outputs=[map_entrega_simulacion,logs_red_state_m,KPIm_node_sobrecargado,KPIm_node_subcargado,KPIm_arista_sobrecargada,KPIm_arista_subcargada])\n",
    "                        btn_bajar_carga.click(make_daily_movements_with_percentages_negative,inputs=[upload_file,dropdown_column_start,dropdown_column_arrival,dropdown_column_quantity,valor_porcentaje],\n",
    "                                              outputs=[map_entrega_simulacion,logs_red_state_m,KPIm_node_sobrecargado,KPIm_node_subcargado,KPIm_arista_sobrecargada,KPIm_arista_subcargada])\n",
    "\n",
    "\n",
    "                    inputs_edge = [source_vertex_id_edge,target_vertex_id_edge] + [atributes_edges[e] for e in attributes_edges_list]\n",
    "                    \n",
    "                    inputs_nodes = [new_node_id] + [atributes_nodes[e] for e in attributes_nodes_list]\n",
    "                    \n",
    "                    def add_edge_with_values_wrapper(source, target, *args):\n",
    "                        attributes = {e: arg for e, arg in zip(attributes_edges_list, args)}\n",
    "                        return add_edge_with_values(source, target, attributes)\n",
    "\n",
    "                    btn_modif_nodes.click(fn=add_node_with_values, inputs=inputs_nodes, outputs=[logs,map_entrega_simulacion])\n",
    "                    btn_modif_edge.click(fn=add_edge_with_values_wrapper, inputs=inputs_edge, outputs=[logs,map_entrega_simulacion])\n",
    "                    btn_erase_node.click(fn=erase_node, inputs=[source_vertex_id], outputs=[logs,map_entrega_simulacion])\n",
    "                    btn_erase_edge.click(fn=erase_edge, inputs=[source_vertex_id,target_vertex_id], outputs=[logs,map_entrega_simulacion])\n",
    "                    btn_reload_map_original.click(fn=reset_html_800,outputs=[map_entrega_original])\n",
    "                    btn_reload_map_simul.click(fn=reset_html1_800,outputs=[map_entrega_simulacion])\n",
    "\n",
    "                demo.launch(max_threads=1000)\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    lis = GradioMetro()\n",
    "    lis.front_func()\n",
    "\n",
    "\n",
    "    #Reactualizar los colores cuando se hace modif de arrista despues de la entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
